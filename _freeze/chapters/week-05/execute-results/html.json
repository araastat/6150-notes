{
  "hash": "f07ceda39ce69c3b539a96e8f7c3eb39",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\ncode-fold: true\nfilters:\n  - webr\n---\n\n\n\n\n\n# Survival analysis I {#sec-week5}\n\n**Last updated:** 05 Oct\\, 2024 12:01 AM EDT\n\n\n\n\n\n\n\n\n\n\n\n::: callout-warning\nWork in progress, target time 9/19 noon\n:::\n\n## Introduction\n\nA common interest across a wide range of fields is to understand when and why\nthings fail. In engineering, that can mean how long will a component of a car\nlast with normal use. In healthcare, that can mean how long does a person with\ndiabetes live. For many businesses, it can mean how long does an employee stay\nwith the company, or how long does a subscriber stay with a plan. The general\nquestion is, how long before something (an *event*) happens. The adjacent\nquestion is, what affects how long before the event happens. Does metformin help\na diabetic prolong the time before blindness and neuropathy set it? Does some\nincentive tend to keep subscribers subscribed longer? Does the Mediterranean\ndiet increase the time until one might get a heart attack?\n\nLooking at failure times and its influencers spawned a different way of using\nstatistics and data, called *survival analysis* in the statistics/biomedical\nworlds and *reliability* in engineering, and *churn analytics* in business.\n\nThere are several things that make survival (which we'll use here instead of the\nmore general *failure time*) data unique.\n\n-   First is that it is always considered to be positive, since we have a time 0\n    when we start measuring time. This leads to particular choices in terms of\n    the probability distributions that we consider when modeling survival\n    analysis.\n\n-   Second is that we often have to deal with missing data in a very particular\n    sense, or perhaps data with incomplete information is a better way to put\n    it. In practice, we can't follow study units (humans, components, customers)\n    forever, and so we stop tracking at some time point. At that point, several\n    units might not have experienced failure yet, but we would reasonably\n    believe that they would fail if we followed them long enough. So we only\n    know, for these units, that they did not fail until the time at which we\n    stopped following them. That *is* information, but it's not the full\n    information we would like. Survival analysis methods account for this\n    partial information issue, which is formally called **censoring**. We'll\n    talk about this in more detail in a bit\n\n    -   A similar issue in survival data is **truncation**. Suppose we're doing\n        a study where we follow patients from the start of their leukemia\n        diagnosis to death. Some patients my enter the study from another\n        hospital, where their diagnosis has happened, but they entered the study\n        some time after. Their \"clock\" on the study started later than 0, and we\n        consider their times *truncated*.\n\n-   Third is how we model covariates that affect (or perhaps cause) changes in\n    survival times. Survival analysis uses statistics and metrics that are not\n    used in other branches of statistics, thus requiring a bit of learning and a\n    re-evaulation of our intuition.\n\n-   Fourth is that the descriptive statistics and graphs we use are different\n    from what you may have learned, both in terms of describing the data and in\n    terms of evaluating model goodness-of-fit and residual analyses. Things get\n    a bit specialized.\n\nIn this chapter we approach the different ways of describing and modeling\nsurvival time itself, including reasonable probability distributions, statistics\nand how we look at changes or differences in survival curves. The next chapter\nwill explore how we evaluate how features/covariates can affect survival times,\nand how we model that effect and evaluate how well our models do. We will focus\non approaches that look at survival time as continuous, and acknowledge that\napproaches that use discretized time do exist and can also be useful.\n\n## Survival data\n\nWe start by following a cohort of patients from the time of admission into a\nstudy and see when they die. As the weeks go by, some patients die, and some\npatients leave the study either by choice or are *lost to followup*. At 12 weeks\nwe stop the study and know that some of the patients we followed are still alive\nthen. We can show this data graphically. [Survival analysis generally is done on\n**prospective** data]{.aside}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(survival)\n\ndata(leukemia) # Acute Myelogeneous Leukemia data\n\nleukemia |> \n  arrange(time) |> \n  mutate(start = 0, ind = 1:n()) |> \n  mutate(status = ifelse(status == 0, 'Censored', \"Died\")) |> \n  ggplot() + \n    geom_segment(aes(x = start, y = ind, xend = time, yend = ind))+\n    geom_point(aes(x = time, y = ind, color = status, shape = status), size=4) + \n    labs(x = \"Weeks\", y = \"\") + \n    theme_bw() + \n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n```\n\n::: {.cell-output-display}\n![Follow-up time in a leukemia cohort, sorted by time of follow-up](week-05_files/figure-html/week-05-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: callout-tip\nThe time when we start counting time, so-called time 0, may not always be\nobvious, and in an actual study may need to be defined. In many clinical\nstudies, time starts either when the disease is diagnosed or when the patient is\nadmitted to hospital. However, as you can imagine, that start time is actually\ndifferent for different patients. We typically align the times so that the\nagreed-upon start time for time counting is always denoted as zero, regardless\nof what calendar time it occurs.\n\n![In a hypothetical clinical study, we see how followup actually occurs in the\nleft panel, where the dotted line denotes when the study stopped. Analytically,\nhowever, we transform the same observations on the study time scale to align all\nstart times to 0, as in the right panel. Here, censoring is represented as an\nopen circle. *From Aalen, Borgan and Gjessing,\n2008*.](images/clipboard-897485810.png)\n:::\n\n### Risk sets\n\nAt any given time $t$, the risk set is defined as the group of individuals who are still at risk _in the study cohort_ to experience an event. This means, we include in the risk set at time $t$ everyone in the study that we are still following at time $t$. \nAnyone who has experienced an event before time $t$, or individuals who are censored before time $t$, are **not** included in the risk set at time $t$. \n\nA risk set is defined at every time during the period of the study. \n\n### Censoring\n\nThere are generally 3 kinds of censoring that we consider:\n\n1.  Right censoring, where we know that a subject has survived up to a certain\n    time, but we don't observe the time of the survival event. This is by far\n    the most common form of censoring and what most analyses account for.\n2.  Interval censoring, where we know that the event occurred between two time\n    points, but we don't know the exact time of the event. This is common for\n    many practical studies, where, for example, we only know that cancer\n    recurrence happened between two visits. Interval censoring can be important\n    to account for in analysis, but it is often ignored if the time intervals\n    between observations are reasonably small.\n3.  Left censoring, where we know that the event occurred before a certain\n    point. For example, we find out a person has died because he didn't show up\n    to his next scheduled doctor's visit.\n\n![From Turkson, Aylah-Mensah & Nimoh (2021).\nhttps://doi.org/10.1155/2021/9307475](images/clipboard-3255178959.png){fig-align=\"center\"}\n\nA fundamental assumption that we make in incorporating censoring is that\n\n> the censoring is independent of the survival outcome\n\nthat is, whether you get censored or not, either during the followup period\n(lost to followup) or at the end of the study (administrative censoring), does\nnot affect whether or not you experience the event, i.e. *noninformative\ncensoring*. This is often seen as a reasonable assumption, especially if\ncensoring happens for reasons extraneous to the disease under study, like, for\nexample, the study stops on a certain date.\n\nThere are instances where this assumption fails, however. For example, patients\nmight be more likely to drop out of a study if they are too sick to take the\nmedication or go to hospital visits, so their censoring is related to the\nseverity of their illness and likely is associated with their risk of dying.\nThis is why the *independent censoring* assumption must be reasoned and checked.\nThere are some techniques around accounting for such *informative censoring*,\nthat will often include weighting schemes like we saw in the propensity score\nweighting. We'll come back to this in a future section.\n\n### The nature of survival data\n\nSurvival data is actually stored as a **bivariate entity** $(T, \\delta)$ , where\n$T$ denotes the observed survival time and $\\delta$ is a 0-1 variable that\ndenotes whether we observed the event(1) or censoring (0) at the end of the\nobserved survival time. We need both to evaluate survival time.\n\n::: callout-warning\n## Clinical data in CDISC format that can go to the FDA\n\nWhen dealing with data from registered clinical trials, the survival (or\ntime-to-event data, denoted TTE) has to conform to a standard format based on\nthe agreed-upon CDISC standard. In this format, you have a *censoring* variable\n(`CNSR`) rather than an *event* variable which is coded in reverse of the\n$\\delta$ above, i.e. it codes being censored as 1 and having the event as 0. So\none of the first lines of code we write is `cens <- 1-CNSR` to accommodate the\nstandard censoring variable that {{<fa brands r-project >}} or {{<fa brands\npython >}} assume.\n:::\n\n## Survival distributions\n\n### Statistics\n\nWe look at different statistics that capture different aspects of the survival\ndistribution, i.e, the distribution of survival times. In the following we\ndenote the survival time by the *random variable* T, which has some distribution\non the support $T \\geq 0$.\n\nSurvival function\n\n:   $S(t) = P(T > t) = 1 - P(T \\leq t) = 1 - F(t)$ for $t \\geq 0$ is the\n    probability that individual survives for at least time t\n\nHazard function\n\n:   The hazard function $h(t)$ is the instantaneous chance that you will fail at\n    time t given you have survived till then.\n    $h(t) = P(T = t | T \\geq t) = \\frac{f(t)}{S(t)}$ where $f(t)$ is the p.d.f\n    of the survival distribution ($f(t) = \\frac{dF(t)}{dt}$). It can also be\n    defined as $h(t) = -\\frac{d}{dt} \\ln(S(t))$.\n\nCumulative hazard function\n\n:   The cumulative hazard function $H(t) = -\\ln(S(t)) = \\int_0^\\infty h(u)du$\n    is, essentially, the sum of all the chances of failing over the period of\n    observation. Mathematically, we can turn this around to show that\n    $S(t) = e^{-H(t)}$\n\nIt turns out that the hazard function is the one we actually work with most of\nthe time in terms of modeling, but we look at the survival time more\ndescriptively.\n\n### Distributions\n\nWe use several distributions to model survival time. These include the\nexponential, Weibull and Gamma models. Details of these distributions are\navailable in @sec-tech. For the purposes of survival analysis, these properties\nof the Exponential and Weibull distributions are\n\n| Parameter         | Exponential                     | Weibull                                          |\n|--------------------|-----------------------|--------------------------------------|\n| Hazard rate       | $h(t) = \\lambda$                | $h(t) = \\lambda k t^{k-1}$                       |\n| p.d.f.            | f(t) = $\\lambda e^{-\\lambda t}$ | f(t) = $\\lambda k t^{k-1} e^{-\\lambda t^k}$      |\n| Survival function | S(t) = $e^{-\\lambda t}$         | S(t) = $e^{-\\lambda t^k}$                        |\n| Mean              | $\\mu = 1/\\lambda$               | $\\mu = \\frac{\\Gamma(1+1/k)}{\\lambda^k}$          |\n| Median            | $-\\log(0.5)/\\lambda$            | $-\\left(\\frac{\\log (0.5)}{\\lambda}\\right)^{1/k}$ |\n\nWe can consider $\\lambda$ as the *rate* at which events occur. A larger\n$\\lambda$ means that the average survival time goes down, which means you have\nmore events per unit time. A really nice interpretation of $k$ is available\n[here](https://en.wikipedia.org/wiki/Weibull_distribution).\n\nOne property of this that derives from the survival function of the Weibull\ndistribution uses the complementary log function (@eq-cll)\n\n$$\n\\log (- \\log S(t)) = \\log \\lambda + k\\log t\n$$ {#eq-cll}\n\nSo the complementary log transformation (log(-log(t)) is linear in log t, which\ncan serve as a test for the adequacy of the Weibull model in fitting the data. [Note\nthat the Exponential is the Weibull with k=1]{.aside}.\n\n::: callout-note\n## More general property{{< iconify mdi:thought-bubble size=Large >}}\n\n**Theorem:** If T is a continuous non-negative random variable with cumulative\nhazard function $\\Lambda$. Then the random variable $Y = \\Lambda(T)$ follows an\nexponential distribution with rate $\\lambda = 1$.\n\nThis property allows us to plot a quantile-quantile plot of **the modelled** $\\Lambda(T)$ against\nan exponential(1) distribution to see how well the model fit is.\n\n```{webr-r week-05-3}\n#| fig-width: 3\nset.seed(20348)\nlibrary(ggplot2)\nlibrary(dplyr)\nd <- tibble(\n    T = rweibull(1000, 5, scale=3) # Simulated survival times (no censoring). FEEL FREE TO PLAY HERE\n  ) |> \n  arrange(T) |> \n  mutate(\n    Ft = (1:n())/n(), # empirical c.d.f.\n    St = 1-Ft,        # empirical survival function\n    Lt = -log(St)    # empirical hazard function\n    )\nggplot(d, aes(sample = Lt))+\n  geom_qq(distribution = stats::qexp) + \n  geom_qq_line(distribution = stats::qexp) + \n  theme_bw() + \n  labs(x = \"Exponential(1) quantiles\", y = \"Empirical cumulative hazard function\")+\n  coord_equal()\n\n```\n:::\n\n### Simulating survival data with censoring\n\nBeing able to simulate survival distributions is often quite useful. We saw\nabove how to simulate some survival times based on non-negatve distributions.\nHowever, in reality, we will encounter right-censoring at the very least. For\nthis, we need to understand further the nature of survival data.\n\nEach observation is associated with two times: the survival time $T^*_i$ and the\ncensoring time $C_i$. What we actually observe is\n\n$$\nT_i = \\min(T^*_i, C_i)\\\\\n\\delta_i = \\begin{cases}\n1 & T_i^* \\leq C_i\\\\\n0 & T_i^* > C_i\n\\end{cases}\n$$ We can simulate data in a similar fashion\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2393)\nnsim <- 1000\nd <- tibble(\nTstar = rweibull(nsim, 0.8), # Survival distribution\ncensor = runif(nsim)*3) |>                     # Censoring distribution\n  mutate(T = pmin(Tstar, censor),\n         delta = ifelse(Tstar <= censor, 1, 0)) |> \n  mutate(indx = 1:n()) |> \n  mutate(delta = ifelse(delta == 1, \"Event\", \"Censored\"))\n\nggplot(d[1:10,]) + \n  geom_segment(aes(x=0, xend = T, y = indx, yend=indx))+\n  geom_point(aes(x = T, y = indx, fill = factor(delta)), shape = 21, size=2) +\n  scale_x_continuous('Time')+\n  scale_y_continuous(\"\")+\n  scale_fill_manual(\"Status\", values = c('white','black'))+\n  theme_bw() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## The survival curve\n\nThe survival curve for a population is a graph plotting $S(t)$ against $t$,\nwhere you can read off, for any time point, the proportion of the population\nstill alive at that time point; conversely for any particular quantile of\nsurvival, you can read off the time to reach that quantile (for example median\nsurvival when 50% of the population remains.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntime <- seq(0,48)\nlambda <- 0.1\ny <- exp(-lambda*time)\ntibble(time, y) |> \n  ggplot(aes(time, y))+\n  geom_line() + \n  scale_y_continuous(\"Probability of survival\", labels = scales::label_percent()) +\n  theme_bw() + labs(title = 'Exponential(0.1) distribution')\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\nGenerally speaking, **if we don't have censoring**, we can generate the\nempirical survival curves, quite easily.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tibble(\n  T = rexp(100, 2)\n) |> \n  arrange(T) |> \n  mutate(St = 1-(1:n())/n())\nggplot(d, aes(T, St))+geom_line() + \n  scale_x_continuous(\"Time\")+\n  scale_y_continuous(\"Survival probability\", labels = scales::label_percent())\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\nIf there is censoring, let's see what happens if we ignore the censoring and\njust plot the reported survival times.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |> mutate(cens = rexp(100,1), \n            Tobs = pmin(T, cens),\n            delta = T <= cens) |> \n  arrange(Tobs) |> \n  mutate(St1 = 1-(1:n())/n()) |> \n  ggplot(aes(Tobs, St1))+geom_line() +\n  geom_line(aes(T, St), color = 'red')+\n  geom_line(aes(x = T, y = pexp(T, 2, lower.tail=FALSE)), size=2,color = 'orange', linetype=2) + labs(x = 'Time', y = 'Probability of survival')\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\nWe find that ignoring censoring (black line) **biases our estimates** of\nsurvival compared to a no censoring situation (red line), which aligns with the\ngenerative model (orange dashed line). This is reasonable since for the censored\nobservations, we're counting them as having the event at that time rather than\nhaving the event *after* that time, and so inflate the failure rates.\n\n### Estimation: parametric models\n\nGenerally speaking, parametric models are fit via maximum likelihood, where the\nlikelihood function is\n\n$$\nL(\\theta|T_i) = \\Pi f(t_i | \\theta)^{\\delta_i} S(t_i)^{1-\\delta_i}\n$$\n\nSo, for example, if we assume an exponential model, we would find $\\lambda$ to\nmaximize\n\n$$\nL(\\lambda | T) = \\Pi_{i=1}^n (\\lambda e^{-\\lambda t_i})^{\\delta_i} (e^{-\\lambda t_i})^{(1-\\delta_i)}\n$$\n\nWe simulate data from an Exponential(0.8) distribution and add censoring. We then do a grid search to find the maximum value of the likelihood function described above.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2393)\nnsim <- 1000\nd <- tibble(\nTstar = rexp(nsim, 0.8), # Survival distribution\ncensor = runif(nsim)*3) |>                     # Censoring distribution\n  mutate(T = pmin(Tstar, censor),\n         delta = ifelse(Tstar <= censor, 1, 0)) |> \n  mutate(indx = 1:n()) |> \n  mutate(delta = ifelse(delta == 1, \"Event\", \"Censored\"))\n\nlik_fn <- function(lambda, d, times=\"T\",cens = \"cens\"){\n  times = d[[times]]\n  cens = as.integer(as.factor(d[[cens]]))==2 # quirky conversion, basically higher level\n  map_dbl(lambda, \\(l) sum(log(dexp(times[cens], rate=l))) + sum(log(pexp(times[!cens], rate=l, lower.tail=F))))\n}\n \n# Here we do grid search to find the maximum of the function\nlmda = seq(0.01, 1, by = 0.01)\nlmda_hat = lmda[which.max(lik_fn(lmda, d, times = 'T', cens = 'delta'))]\n```\n:::\n\n\n\n\n\nThe optimization gives the estimate of the exponential parameter as\n0.83. \n\nAnother way of approaching the problem that uses the same method\nuses the `survreg` function, which we'll see more of next week as we look at survival regression.Note, in R, we use the function `Surv` to incorporate the bivariate survival\ndata into models.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\nfit <- survreg(Surv(T, delta=='Event') ~ 1, data = d, dist = 'exponential')\n```\n:::\n\n\n\n\nThe estimate we get from here is 0.831.\n\n::: aside\nA note here on notation. Here we're only trying to estimate one survival curve,\nso the right side of the equation is `1`. We'll see in a bit how that gets\nmodified when we start looking at groups.\n:::\n\nWe can also plot the predicted survival curve from the `survreg` package.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nprobs <- seq(0.01, 1, by = 0.01)                         # Probability of death\npred_times <- predict(fit,type = 'quantile', p = probs)  # predicted times\ntibble(\n  probs = 1-probs,  # convert to survival probability\n  times = pred_times[1,]\n) |> \n  ggplot(aes(x = times, y = probs))+geom_line()\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Estimation: non-parametric methods\n\nThe most common way to estimate the survival curve is the **Kaplan-Meier\ncurve**, which provides an unbiased estimate of the survival curve in the\npresence of independent censoring. It appears as a step function which is\nconstant between failure times, and the vertical jump at each failure time is\ninversely proportional to the number of individuals still at risk at that time.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\nsfit <- survfit(Surv(T, delta=='Event') ~ 1, data = d)\nplot(sfit)\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\nA more refined (and personally preferred) version is using the **ggsurvfit**\npackage which is aligned with **ggplot2**. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggsurvfit)\nsurvfit2(Surv(T, delta=='Event')~1, data = d) |> \n  ggsurvfit() + add_risktable()\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Aligning the different estimates\n\nWe see that accounting for censoring reduces the bias in estimating the survival curve. We've generated the data from an exponential model, so the parametric method does very well (it uses the exponential to fit). The Kaplan-Meier method is known to generate unbiased estimates generally as long as the censoring is non-informative. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(geomtextpath)\nlibrary(ggsurvfit)\nplt_data <- tibble(times = d$T, cens = d$delta=='Event') |> \n  arrange(times) |> mutate(St = 1-(1:n())/n())\nggdata = survfit2(Surv(times, cens)~1, data=plt_data) |> tidy_survfit()\n\nggplot(plt_data) + \n  geom_line(data = plt_data, aes(x = times, y = pexp(times, rate = 0.8, lower.tail = FALSE), color = \"Truth\"), linewidth=2)+\n  geom_line(data = plt_data, aes(x = times, y =pexp(times, rate = lmda_hat, lower.tail=FALSE), color = \"Parametric\"),  linewidth=1.5) + \n  geom_step(data = ggdata, aes(x = time, y = estimate, color = 'KM'), linewidth=1.5) +\n  geom_line(aes(times, St, color = 'Ignore censoring' ), linewidth=1.5) + \n  labs(x = \"Time\", y = \"Survival probability\")+\n  scale_color_manual(name = \"Method\", values = c(\"Truth\" = 'green', 'Parametric'='orange', \"KM\" = 'blue', \"Ignore censoring\" = 'red'))\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Different statistics\n\nThere are different metrics that can help summarise a survival distribution for the purposes of comparing different distributions or reporting landmarks. \n\n### Median survival time\n\nIn the literature, the statistic we see most commonly is the _median survival time_. Due to censoring, we also have to be careful in estimating it, and need to use censoring-aware methods like the previous section, since it is derived from the estimated survival curve. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsfit <- survfit2(Surv(T, delta==\"Event\") ~ 1, data = d) # can also use survfit\nas_tibble(quantile(sfit, probs = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  quantile lower upper\n     <dbl> <dbl> <dbl>\n1    0.814 0.742 0.888\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggsurvfit(sfit)+\n  add_quantile(y_value = 0.5)\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Mean survival time\n\nThe mean survival time seems an attractive metric; how long on average to subjects in the population survive? However, when we want to estimate mean survival time in the presence of censoring, we run into problems. In particular, if the survival curve doesn't touch the x-axis, which means that there remain some censored individuals at the end of followup, the mean survival time remains undefined, since theoretically the unknown survival times could be $\\infty$. \n\nWhat can be computed is the **restricted mean survival time (RMST)**, which asks what is the average survival time during a defined time period. It is \"computed\" as the area under the survival curve up to some specified time. We will use the {{< fa brands r-project >}} package **RISCA** to compute the RMST.[`install.packages('RISCA')`]{.aside}. This statistic is useful for comparing two survival curves, as we'll see in a bit. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(RISCA)\nsdata <- broom::tidy(sfit)\nggplot(sdata, aes(time, estimate)) + geom_step()+\n  geom_area(data = sdata |> filter(time <= 2), fill = 'blue')\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/week-05-16-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nwith(sdata, rmst(time, estimate, max.time = 2, type = 's'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9730756\n```\n\n\n:::\n:::\n\n\n\n\n\n### Landmark estimate\n\nWe're often interested in the proportion of units who have survived to a particular time. In drug trials this is often 1-, 2- or 5-year survival. This is a binary outcome of yes (survived to the landmark time) or no (had an event before the landmark time).\n\n::: {.callout-important}\nBe careful with the definition above. An error is commonly made with creating the outcome variable. Only individuals in the risk set at the landmark time receive a \"no\" or 0, and only individuals who have experienced an event at or before the landmark time are given a \"yes\" or 1. Anyone who has been censored before the landmark time is given a missing value (`NA`), since we don't know what their outcome status at the landmark time is. \n:::\n\nThe landmark estimate is the proportion of 'yes'/1s in the variable described above, ignoring the observatins that are marked as `NA`. To be precise, \n\n::: callout-note\nLandmark estimate = (# with an event at or before the landmark time)/(# who were not censored before the landmark time)\n\nThe denominator inludes everyone in the numerator + everyone in the [risk set](#risk-sets) at the landmark time. \n:::\n\n## Comparing survival\n\nA central aspect of survival analysis is to compare the survival experience of patients who are exposed to different treatments or have different genetic profiles. The inference that we wish to make is that survival under one condition is longer than under another condition. \n\nWhen we want to see if a new drug is better than existing treatments, we first design a randomized clinical trial where subjects are randomly assigned to the new treatment (T) or the standard of care (C). We then follow subjects going forward until the planned end of the study and observe the planned outcome (which can be disease recurrence, death, progression of disease or something else). Some individuals end up being censored due to drop-out or remaining alive at the end of the study. For each group of patients (which are called treatment arms), we can construct a survival curve, perhaps using the Kaplan-Meier technique. So what does it mean to say that indivudals on treatment have better survival than people on standard of care?\n\nThere are two broad approaches to answering this question. \n\n- The first is **landmark analysis**, where we see how patients are doing after a certain amount of time on the study, often 1-, 2- or 5-years. \n- The second approach is to ask if the overall survival experience for the treatment group is better than that of the standard of care group. \n\n### Landmark analysis\n\nWe saw [earlier](#landmark-estimate) how to compute the landmark estimate. For each arm of a trial (or each subgroup/stratum in the study), you can compute the landmark estimate for that arm. The landmark estimate is a proportion, and so we can use a two-sample test for proportions to evaluate the difference between the landmark estimates from two arms. \n\nLet's look at an example of doing this using the in-built dataset `colon` from a clinical trial of adjuvant chemotherapy in Stage B/C colon cancer. We will compare the 2 year overall survival between different treatment arms. We first compute the 2-year overall survival fo each of the three arms. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(survival)\ncolon_os <- colon |> dplyr::filter(etype == 2)\ncolon_os |>\n  filter((status == 1 & time <= 2*365) | (status == 0 & time >= 2*365)) |>\n  group_by(rx) |>\n  summarise(prop = sum(1-status)/n())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  rx       prop\n  <fct>   <dbl>\n1 Obs     0.661\n2 Lev     0.665\n3 Lev+5FU 0.751\n```\n\n\n:::\n:::\n\n\n\n\n\nWe will now test if the Lev+5FU arm does better than the Lev arm. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(infer)\ntest_dat <- colon_os |>\n  filter((status == 1 & time <= 2*365) | (status == 0 & time >= 2*365)) |>\n  filter(rx != 'Obs') |>\n  mutate(rx = droplevels(rx),\n  status = factor(1-status)) # convert to survival\nobs_stat = test_dat |>\n  specify(status ~ rx, success = '1') |> \n  calculate('diff in props', order = c('Lev+5FU', 'Lev'))\ntest_dat |> specify(status ~ rx, success = '1') |>\n  hypothesize(null = 'independence') |>\n  generate(reps =5000, 'permute') |>\n  calculate('diff in props', order = c('Lev+5FU', 'Lev')) |>\n  get_p_value(obs_stat, 'right') # one-sided (greater) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1  0.0242\n```\n\n\n:::\n:::\n\n\n\n\n\n::: callout-note\nTry to see whether there is a significant difference at 5 years.\n:::\n\n### The hazard function and the proportional hazards assumption\n\n\nWhat we've set out to do is to see if two survival curves are different in some sense. Intuitively we want to see two survival curves with one entirely above the other, to have a clear case that the survival experience in one group is better than that of the other. \n\nLet's look at the exponential distribution. The exponential distribution is characterised by a single parameter, the hazard. Suppose the two groups have hazards $\\lambda_1$ and $\\lambda_2$. If $\\lambda_1 > \\lambda_2$, then we will have $\\exp(-\\lambda_1 t) < \\exp(-\\lambda_2 t)$ or $S(t|\\lambda_1) < S(t|\\lambda_2)$\n\n\n\n\nSo, for an exponential distribution, the hazard, which is a constant, characterises the distribution, and if the hazard increases, we see the survival function decreases uniformly at all times, making a clear case that survival in one group is clearly better than the other for all times $t$.\n\nThis idea can be extended. It turns out, if the hazards of two survival curves are **proportional**, i.e. the ratio of the hazards is a constant independent of time, then one survival curve will be uniformly above the the other curve and the superiority of one survival experience is clear. This is called the **proportional hazards assumption**\n\n::: {.callout-note}\n## Digging a bit into the the PH assumption {{< iconify mdi:thought-bubble size=Large >}}\n\nWe recall that $S(t) = \\exp\\left(-\\int_0^\\infty h(u)du\\right)$. If the two groups have hazard functions $h_1(t)$ and $h_2(t)$. If we assume that $h_2(t) = \\gamma h_1(t)$  for some $\\gamma > 1$ (the PH assumption). Then we have\n\n$$\n\\begin{array}{ll}\nS_2(t) &=& \\exp\\left(-\\int_0^\\infty h_2(u)du\\right)\\\\\n&=& \\exp\\left( -\\int_0^\\infty \\gamma h_1(u) du\\right)\\\\\n&=& \\exp\\left( -\\gamma \\int_0^\\infty h_1(u) du\\right)\\\\\n&=& S_1(t)^\\gamma\n\\end{array}\n$$\n\nThe proportional hazards assumption thus implies that one survival curve is uniformly higher than the other. \n\nI'll state that since $\\gamma > 1$, $h_2(t) > h_1(t),\\ \\forall t$ which, in turn, implies $S_2(t) < S_1(t)$. **Convince yourself of this.** The heuristics are quite clear, in that a higher hazard means you have more events per unit time, which means there are more events earlier which makes the survival curve be lower. \n\n:::\n\nThe PH assumption thus allows us to judge relative survival through the hazard functions, in particular, we look at the **hazard ratio** which is independent of time under the PH assumption. We can formulate a hypothesis test in this situation as \n\n$$\nH_0: HR = 1\\ \\text{vs}\\ H_1: HR \\neq 1\n$$\n\nThe formal hypothesis test that tests this hypothesis is called the **log-rank test**, which is a form of $\\chi^2$-test. This test works provided (a) the censoring is uninformative and the rates of censoring are the same in both arms, (b) proportional hazards. We can perform the log-rank test directly in R.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(survival)\ndifftest <- survdiff(Surv(time, status) ~ rx, data = colon |> filter(rx != 'Obs'))\n# difftest$pvalue\n```\n:::\n\n\n\n\n\nWe get a p-value of p<0\\.000001.\n\n::: aside\nThe `survdiff` function exposes the p-value in a slot named \"pvalue\", which is different from the much more common \"p.value\". To maintain uniformity, you can do `broom::tidy(difftest)$p.value`\n:::\n\nThis p-value is obtained under the PH assumption, but we can use resampling to estimate the p-value as well. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(rsample)\nset.seed(2893)\nresamples <- permutations(colon |> filter(rx != \"Obs\"), permute = rx, times = 1000)\nobs_stat <- survdiff(Surv(time, status)~rx, data = colon |> filter(rx != 'Obs'))$chisq\nnull_dist <- map_dbl(resamples$splits, \\(d) survdiff(Surv(time, status)~ rx, data = analysis(d))$chisq)\np.value <- mean(null_dist > obs_stat)\n```\n:::\n\n\n\n\nThe permutation p-value is < 2\\.22e\\-16\n\n### Non-proportional hazards and using RMST for comparison {#sec-nph}\n\nIn order to compare survival curves, we need some overall metric that summarizes the survival curve. We will demonstrate how to use the RMST to compare survival distributions by strata. \n\n:::{.aside}\n`install.packages(\"survRM2\")`\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(survRM2)\nlibrary(gt)\nout = with(colon |> filter(rx != \"Obs\"),\n  rmst2(time = time, status = status, arm = ifelse(rx == \"Lev+5FU\", 1, 0)))\ngt(as_tibble(out$unadjusted.result)[1,]) |>\n  fmt_scientific(columns = 4)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"cbxmkaspyx\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#cbxmkaspyx table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#cbxmkaspyx thead, #cbxmkaspyx tbody, #cbxmkaspyx tfoot, #cbxmkaspyx tr, #cbxmkaspyx td, #cbxmkaspyx th {\n  border-style: none;\n}\n\n#cbxmkaspyx p {\n  margin: 0;\n  padding: 0;\n}\n\n#cbxmkaspyx .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#cbxmkaspyx .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#cbxmkaspyx .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#cbxmkaspyx .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#cbxmkaspyx .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#cbxmkaspyx .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#cbxmkaspyx .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#cbxmkaspyx .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#cbxmkaspyx .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#cbxmkaspyx .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#cbxmkaspyx .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#cbxmkaspyx .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#cbxmkaspyx .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#cbxmkaspyx .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#cbxmkaspyx .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cbxmkaspyx .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#cbxmkaspyx .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#cbxmkaspyx .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#cbxmkaspyx .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cbxmkaspyx .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#cbxmkaspyx .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cbxmkaspyx .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#cbxmkaspyx .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cbxmkaspyx .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#cbxmkaspyx .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#cbxmkaspyx .gt_left {\n  text-align: left;\n}\n\n#cbxmkaspyx .gt_center {\n  text-align: center;\n}\n\n#cbxmkaspyx .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#cbxmkaspyx .gt_font_normal {\n  font-weight: normal;\n}\n\n#cbxmkaspyx .gt_font_bold {\n  font-weight: bold;\n}\n\n#cbxmkaspyx .gt_font_italic {\n  font-style: italic;\n}\n\n#cbxmkaspyx .gt_super {\n  font-size: 65%;\n}\n\n#cbxmkaspyx .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#cbxmkaspyx .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#cbxmkaspyx .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#cbxmkaspyx .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#cbxmkaspyx .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#cbxmkaspyx .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#cbxmkaspyx .gt_indent_5 {\n  text-indent: 25px;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Est.\">Est.</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"lower .95\">lower .95</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"upper .95\">upper .95</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"p\">p</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Est.\" class=\"gt_row gt_right\">396.6133</td>\n<td headers=\"lower .95\" class=\"gt_row gt_right\">245.8437</td>\n<td headers=\"upper .95\" class=\"gt_row gt_right\">547.3829</td>\n<td headers=\"p\" class=\"gt_row gt_right\">2.52 × 10<sup style='font-size: 65%;'>−7</sup></td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\n\nThis gives a confidence interval and p-value based on large-sample (asymptotic) considerations. A better evaluation would be based on permuation testing.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(rsample)\nset.seed(2845)\npermutes <- permutations(\n  colon |> filter(rx != \"Obs\"),\n  permute = rx,\n  times = 10000\n)\nobs_stat <- out$unadjusted.result[1,1]\nnull_dist <- map_dbl(\n  permutes$splits,\n  \\(d) with(analysis(d), rmst2(time = time, status = status, arm = ifelse(rx == \"Lev+5FU\", 1, 0))$unadjusted.result[1,1]\n))\npval <- mean(null_dist >= obs_stat)\nif(pval == 0) pval <- \"< 0.0001\"\n```\n:::\n\n\n\n\nThe permutation-based p-value is < 0\\.0001.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}