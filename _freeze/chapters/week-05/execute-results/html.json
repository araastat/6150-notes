{
  "hash": "13705d38209cc66bf2e660dbbb483b78",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\ncode-fold: true\nfilters:\n  - webr\n---\n\n\n\n\n\n# Survival analysis I {#sec-week5}\n\n**Last updated:** 19 Sep\\, 2024 05:26 PM EDT\n\n\n\n\n\n\n\n\n\n\n\n::: callout-warning\nWork in progress, target time 9/19 noon\n:::\n\n## Introduction\n\nA common interest across a wide range of fields is to understand when and why\nthings fail. In engineering, that can mean how long will a component of a car\nlast with normal use. In healthcare, that can mean how long does a person with\ndiabetes live. For many businesses, it can mean how long does an employee stay\nwith the company, or how long does a subscriber stay with a plan. The general\nquestion is, how long before something (an *event*) happens. The adjacent\nquestion is, what affects how long before the event happens. Does metformin help\na diabetic prolong the time before blindness and neuropathy set it? Does some\nincentive tend to keep subscribers subscribed longer? Does the Mediterranean\ndiet increase the time until one might get a heart attack?\n\nLooking at failure times and its influencers spawned a different way of using\nstatistics and data, called *survival analysis* in the statistics/biomedical\nworlds and *reliability* in engineering, and *churn analytics* in business.\n\nThere are several things that make survival (which we'll use here instead of the\nmore general *failure time*) data unique.\n\n-   First is that it is always considered to be positive, since we have a time 0\n    when we start measuring time. This leads to particular choices in terms of\n    the probability distributions that we consider when modeling survival\n    analysis.\n\n-   Second is that we often have to deal with missing data in a very particular\n    sense, or perhaps data with incomplete information is a better way to put\n    it. In practice, we can't follow study units (humans, components, customers)\n    forever, and so we stop tracking at some time point. At that point, several\n    units might not have experienced failure yet, but we would reasonably\n    believe that they would fail if we followed them long enough. So we only\n    know, for these units, that they did not fail until the time at which we\n    stopped following them. That *is* information, but it's not the full\n    information we would like. Survival analysis methods account for this\n    partial information issue, which is formally called **censoring**. We'll\n    talk about this in more detail in a bit\n\n    -   A similar issue in survival data is **truncation**. Suppose we're doing\n        a study where we follow patients from the start of their leukemia\n        diagnosis to death. Some patients my enter the study from another\n        hospital, where their diagnosis has happened, but they entered the study\n        some time after. Their \"clock\" on the study started later than 0, and we\n        consider their times *truncated*.\n\n-   Third is how we model covariates that affect (or perhaps cause) changes in\n    survival times. Survival analysis uses statistics and metrics that are not\n    used in other branches of statistics, thus requiring a bit of learning and a\n    re-evaulation of our intuition.\n\n-   Fourth is that the descriptive statistics and graphs we use are different\n    from what you may have learned, both in terms of describing the data and in\n    terms of evaluating model goodness-of-fit and residual analyses. Things get\n    a bit specialized.\n\nIn this chapter we approach the different ways of describing and modeling\nsurvival time itself, including reasonable probability distributions, statistics\nand how we look at changes or differences in survival curves. The next chapter\nwill explore how we evaluate how features/covariates can affect survival times,\nand how we model that effect and evaluate how well our models do. We will focus\non approaches that look at survival time as continuous, and acknowledge that\napproaches that use discretized time do exist and can also be useful.\n\n## Survival data\n\nWe start by following a cohort of patients from the time of admission into a\nstudy and see when they die. As the weeks go by, some patients die, and some\npatients leave the study either by choice or are *lost to followup*. At 12 weeks\nwe stop the study and know that some of the patients we followed are still alive\nthen. We can show this data graphically. [Survival analysis generally is done on\n**prospective** data]{.aside}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(survival)\n\ndata(leukemia) # Acute Myelogeneous Leukemia data\n\nleukemia |> \n  arrange(time) |> \n  mutate(start = 0, ind = 1:n()) |> \n  mutate(status = ifelse(status == 0, 'Censored', \"Died\")) |> \n  ggplot() + \n    geom_segment(aes(x = start, y = ind, xend = time, yend = ind))+\n    geom_point(aes(x = time, y = ind, color = status, shape = status), size=4) + \n    labs(x = \"Weeks\", y = \"\") + \n    theme_bw() + \n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n```\n\n::: {.cell-output-display}\n![Follow-up time in a leukemia cohort, sorted by time of follow-up](week-05_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n::: callout-tip\nThe time when we start counting time, so-called time 0, may not always be\nobvious, and in an actual study may need to be defined. In many clinical\nstudies, time starts either when the disease is diagnosed or when the patient is\nadmitted to hospital. However, as you can imagine, that start time is actually\ndifferent for different patients. We typically align the times so that the\nagreed-upon start time for time counting is always denoted as zero, regardless\nof what calendar time it occurs.\n\n![In a hypothetical clinical study, we see how followup actually occurs in the\nleft panel, where the dotted line denotes when the study stopped. Analytically,\nhowever, we transform the same observations on the study time scale to align all\nstart times to 0, as in the right panel. Here, censoring is represented as an\nopen circle. *From Aalen, Borgan and Gjessing,\n2008*.](images/clipboard-897485810.png)\n:::\n\n### Censoring\n\nThere are generally 3 kinds of censoring that we consider:\n\n1.  Right censoring, where we know that a subject has survived up to a certain\n    time, but we don't observe the time of the survival event. This is by far\n    the most common form of censoring and what most analyses account for.\n2.  Interval censoring, where we know that the event occurred between two time\n    points, but we don't know the exact time of the event. This is common for\n    many practical studies, where, for example, we only know that cancer\n    recurrence happened between two visits. Interval censoring can be important\n    to account for in analysis, but it is often ignored if the time intervals\n    between observations are reasonably small.\n3.  Left censoring, where we know that the event occurred before a certain\n    point. For example, we find out a person has died because he didn't show up\n    to his next scheduled doctor's visit.\n\n![From Turkson, Aylah-Mensah & Nimoh (2021).\nhttps://doi.org/10.1155/2021/9307475](images/clipboard-3255178959.png){fig-align=\"center\"}\n\nA fundamental assumption that we make in incorporating censoring is that\n\n> the censoring is independent of the survival outcome\n\nthat is, whether you get censored or not, either during the followup period\n(lost to followup) or at the end of the study (administrative censoring), does\nnot affect whether or not you experience the event, i.e. *noninformative\ncensoring*. This is often seen as a reasonable assumption, especially if\ncensoring happens for reasons extraneous to the disease under study, like, for\nexample, the study stops on a certain date.\n\nThere are instances where this assumption fails, however. For example, patients\nmight be more likely to drop out of a study if they are too sick to take the\nmedication or go to hospital visits, so their censoring is related to the\nseverity of their illness and likely is associated with their risk of dying.\nThis is why the *independent censoring* assumption must be reasoned and checked.\nThere are some techniques around accounting for such *informative censoring*,\nthat will often include weighting schemes like we saw in the propensity score\nweighting. We'll come back to this in a future section.\n\n### The nature of survival data\n\nSurvival data is actually stored as a **bivariate entity** $(T, \\delta)$ , where\n$T$ denotes the observed survival time and $\\delta$ is a 0-1 variable that\ndenotes whether we observed the event(1) or censoring (0) at the end of the\nobserved survival time. We need both to evaluate survival time.\n\n::: callout-warning\n## Clinical data in CDISC format that can go to the FDA\n\nWhen dealing with data from registered clinical trials, the survival (or\ntime-to-event data, denoted TTE) has to conform to a standard format based on\nthe agreed-upon CDISC standard. In this format, you have a *censoring* variable\n(`CNSR`) rather than an *event* variable which is coded in reverse of the\n$\\delta$ above, i.e. it codes being censored as 1 and having the event as 0. So\none of the first lines of code we write is `cens <- 1-CNSR` to accommodate the\nstandard censoring variable that {{<fa brands r-project >}} or {{<fa brands\npython >}} assume.\n:::\n\n## Survival distributions\n\n### Statistics\n\nWe look at different statistics that capture different aspects of the survival\ndistribution, i.e, the distribution of survival times. In the following we\ndenote the survival time by the *random variable* T, which has some distribution\non the support $T \\geq 0$.\n\nSurvival function\n\n:   $S(t) = P(T > t) = 1 - P(T \\leq t) = 1 - F(t)$ for $t \\geq 0$ is the\n    probability that individual survives for at least time t\n\nHazard function\n\n:   The hazard function $h(t)$ is the instantaneous chance that you will fail at\n    time t given you have survived till then.\n    $h(t) = P(T = t | T \\geq t) = \\frac{f(t)}{S(t)}$ where $f(t)$ is the p.d.f\n    of the survival distribution ($f(t) = \\frac{dF(t)}{dt}$). It can also be\n    defined as $h(x) = -\\frac{d}{dt} \\ln(S(t))$.\n\nCumulative hazard function\n\n:   The cumulative hazard function $H(t) = -\\ln(S(t)) = \\int_0^\\infty h(u)du$\n    is, essentially, the sum of all the chances of failing over the period of\n    observation. Mathematically, we can turn this around to show that\n    $S(t) = e^{-H(t)}$\n\nIt turns out that the hazard function is the one we actually work with most of\nthe time in terms of modeling, but we look at the survival time more\ndescriptively.\n\n### Distributions\n\nWe use several distributions to model survival time. These include the\nexponential, Weibull and Gamma models. Details of these distributions are\navailable in @sec-tech. For the purposes of survival analysis, these properties\nof the Exponential and Weibull distributions are\n\n| Parameter         | Exponential                     | Weibull                                          |\n|--------------------|-----------------------|--------------------------------------|\n| Hazard rate       | $h(t) = \\lambda$                | $h(t) = \\lambda k t^{k-1}$                       |\n| p.d.f.            | f(t) = $\\lambda e^{-\\lambda t}$ | f(t) = $\\lambda k t^{k-1} e^{-\\lambda t^k}$      |\n| Survival function | S(t) = $e^{-\\lambda t}$         | S(t) = $e^{-\\lambda t^k}$                        |\n| Mean              | $\\mu = 1/\\lambda$               | $\\mu = \\frac{\\Gamma(1+1/k)}{\\lambda^k}$          |\n| Median            | $-\\log(0.5)/\\lambda$            | $-\\left(\\frac{\\log (0.5)}{\\lambda}\\right)^{1/k}$ |\n\nWe can consider $\\lambda$ as the *rate* at which events occur. A larger\n$\\lambda$ means that the average survival time goes down, which means you have\nmore events per unit time. A really nice interpretation of $k$ is available\n[here](https://en.wikipedia.org/wiki/Weibull_distribution).\n\nOne property of this that derives from the survival function of the Weibull\ndistribution is that\n\n$$\n\\log (- \\log S(t)) = \\log \\lambda + k\\log t\n$$\n\nSo the complementary log transformation (log(-log(t)) is linear in log t, which\ncan serve as a test for the adequacy of the model in fitting the data. [Note\nthat the Exponential is the Weibull with k=1]{.aside}.\n\n::: callout-note\n## More general property{{< iconify mdi:thought-bubble size=Large >}}\n\n**Theorem:** If T is a continuous non-negative random variable with cumulative\nhazard function $\\Lambda$. Then the random variable $Y = \\Lambda(T)$ follows an\nexponential distribution with rate $\\lambda = 1$.\n\nThis property allows us to plot a quantile-quantile plot of $\\Lambda(T)$ against\nan exponential(1) distribution to see how well the model fit is.\n\n```{webr-r}\n#| fig-width: 3\nset.seed(20348)\nlibrary(ggplot2)\nlibrary(dplyr)\nd <- tibble(\n    T = rweibull(1000, 5, scale=3) # Simulated survival times (no censoring). FEEL FREE TO PLAY HERE\n  ) |> \n  arrange(T) |> \n  mutate(\n    Ft = (1:n())/n(), # empirical c.d.f.\n    St = 1-Ft,        # empirical survival function\n    Lt = -log(St)    # empirical hazard function\n    )\nggplot(d, aes(sample = Lt))+\n  geom_qq(distribution = stats::qexp) + \n  geom_qq_line(distribution = stats::qexp) + \n  theme_bw() + \n  labs(x = \"Exponential(1) quantiles\", y = \"Empirical cumulative hazard function\")+\n  coord_equal()\n\n```\n:::\n\n### Simulating survival data with censoring\n\nBeing able to simulate survival distributions is often quite useful. We saw\nabove how to simulate some survival times based on non-negatve distributions.\nHowever, in reality, we will encounter right-censoring at the very least. For\nthis, we need to understand further the nature of survival data.\n\nEach observation is associated with two times: the survival time $T^*_i$ and the\ncensoring time $C_i$. What we actually observe is\n\n$$\nT_i = \\min(T^*_i, C_i)\\\\\n\\delta_i = \\begin{cases}\n1 & T_i^* \\leq C_i\\\\\n0 & T_i^* > C_i\n\\end{cases}\n$$ We can simulate data in a similar fashion\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2393)\nnsim <- 1000\nd <- tibble(\nTstar = rweibull(nsim, 0.8), # Survival distribution\ncensor = runif(nsim)*3) |>                     # Censoring distribution\n  mutate(T = pmin(Tstar, censor),\n         delta = ifelse(Tstar <= censor, 1, 0)) |> \n  mutate(indx = 1:n()) |> \n  mutate(delta = ifelse(delta == 1, \"Event\", \"Censored\"))\n\nggplot(d[1:10,]) + \n  geom_segment(aes(x=0, xend = T, y = indx, yend=indx))+\n  geom_point(aes(x = T, y = indx, fill = factor(delta)), shape = 21, size=2) +\n  scale_x_continuous('Time')+\n  scale_y_continuous(\"\")+\n  scale_fill_manual(\"Status\", values = c('white','black'))+\n  theme_bw() +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank())\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## The survival curve\n\nThe survival curve for a population is a graph plotting $S(t)$ against $t$,\nwhere you can read off, for any time point, the proportion of the population\nstill alive at that time point; conversely for any particular quantile of\nsurvival, you can read off the time to reach that quantile (for example median\nsurvival when 50% of the population remains.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntime <- seq(0,48)\nlambda <- 0.1\ny <- exp(-lambda*time)\ntibble(time, y) |> \n  ggplot(aes(time, y))+\n  geom_line() + \n  scale_y_continuous(\"Probability of survival\", labels = scales::label_percent()) +\n  theme_bw() + labs(title = 'Exponential(0.1) distribution')\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\nGenerally speaking, **if we don't have censoring**, we can generate the\nempirical survival curves, quite easily.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- tibble(\n  T = rexp(100, 2)\n) |> \n  arrange(T) |> \n  mutate(St = 1-(1:n())/n())\nggplot(d, aes(T, St))+geom_line() + \n  scale_x_continuous(\"Time\")+\n  scale_y_continuous(\"Survival probability\", labels = scales::label_percent())\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\nIf there is censoring, let's see what happens if we ignore the censoring and\njust plot the reported survival times.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |> mutate(cens = rexp(100,1), \n            Tobs = pmin(T, cens),\n            delta = T <= cens) |> \n  arrange(Tobs) |> \n  mutate(St1 = 1-(1:n())/n()) |> \n  ggplot(aes(Tobs, St1))+geom_line() +\n  geom_line(aes(T, St), color = 'red')+\n  geom_line(aes(x = T, y = pexp(T, 2, lower.tail=FALSE)), size=2,color = 'orange', linetype=2) + labs(x = 'Time', y = 'Probability of survival')\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\nWe find that ignoring censoring (black line) **biases our estimates** of\nsurvival compared to a no censoring situation (red line), which aligns with the\ngenerative model (orange dashed line). This is reasonable since for the censored\nobservations, we're counting them as having the event at that time rather than\nhaving the event *after* that time, and so inflate the failure rates.\n\n### Estimation: parametric models\n\nGenerally speaking, parametric models are fit via maximum likelihood, where the\nlikelihood function is\n\n$$\nL(\\theta|T_i) = \\Pi f(t_i | \\theta)^{\\delta_i} S(t_i)^{1-\\delta_i}\n$$\n\nSo, for example, if we assume an exponential model, we would find $\\lambda$ to\nmaximize\n\n$$\nL(\\lambda | T) = \\Pi_{i=1}^n (\\lambda e^{-\\lambda t_i})^{\\delta_i} (e^{-\\lambda t_i})^{(1-\\delta_i)}\n$$\n\nWe simulate data from an Exponential(0.8) distribution and add censoring. We then do a grid search to find the maximum value of the likelihood function described above.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2393)\nnsim <- 1000\nd <- tibble(\nTstar = rexp(nsim, 0.8), # Survival distribution\ncensor = runif(nsim)*3) |>                     # Censoring distribution\n  mutate(T = pmin(Tstar, censor),\n         delta = ifelse(Tstar <= censor, 1, 0)) |> \n  mutate(indx = 1:n()) |> \n  mutate(delta = ifelse(delta == 1, \"Event\", \"Censored\"))\n\nlik_fn <- function(lambda, d, times=\"T\",cens = \"cens\"){\n  times = d[[times]]\n  cens = as.integer(as.factor(d[[cens]]))==2 # quirky conversion, basically higher level\n  map_dbl(lambda, \\(l) sum(log(dexp(times[cens], rate=l))) + sum(log(pexp(times[!cens], rate=l, lower.tail=F))))\n}\n \n# Here we do grid search to find the maximum of the function\nlmda = seq(0.01, 1, by = 0.01)\nlmda_hat = lmda[which.max(lik_fn(lmda, d, times = 'T', cens = 'delta'))]\n```\n:::\n\n\n\n\n\nThe optimization gives the estimate of the exponential parameter as\n0.83. \n\nAnother way of approaching the problem that uses the same method\nuses the `survreg` function, which we'll see more of next week as we look at survival regression.Note, in R, we use the function `Surv` to incorporate the bivariate survival\ndata into models.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\nfit <- survreg(Surv(T, delta=='Event') ~ 1, data = d, dist = 'exponential')\n```\n:::\n\n\n\n\nThe estimate we get from here is 0.831.\n\n::: aside\nA note here on notation. Here we're only trying to estimate one survival curve,\nso the right side of the equation is `1`. We'll see in a bit how that gets\nmodified when we start looking at groups.\n:::\n\n### Estimation: non-parametric methods\n\nThe most common way to estimate the survival curve is the **Kaplan-Meier\ncurve**, which provides an unbiased estimate of the survival curve in the\npresence of independent censoring. It appears as a step function which is\nconstant between failure times, and the vertical jump at each failure time is\ninversely proportional to the number of individuals still at risk at that time.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\nsfit <- survfit(Surv(T, delta=='Event') ~ 1, data = d)\nplot(sfit)\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\nA more refined (and personally preferred) version is using the **ggsurvfit**\npackage which is aligned with **ggplot2**. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggsurvfit)\nsurvfit2(Surv(T, delta=='Event')~1, data = d) |> \n  ggsurvfit() + add_risktable()\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Aligning the different estimates\n\nWe see that accounting for censoring reduces the bias in estimating the survival curve. We've generated the data from an exponential model, so the parametric method does very well (it uses the exponential to fit). The Kaplan-Meier method is known to generate unbiased estimates generally as long as the censoring is non-informative. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(geomtextpath)\nlibrary(ggsurvfit)\nplt_data <- tibble(times = d$T, cens = d$delta=='Event') |> \n  arrange(times) |> mutate(St = 1-(1:n())/n())\nggdata = survfit2(Surv(times, cens)~1, data=plt_data) |> tidy_survfit()\n\nggplot(plt_data) + \n  geom_line(data = plt_data, aes(x = times, y = pexp(times, rate = 0.8, lower.tail = FALSE), color = \"Truth\"), linewidth=2)+\n  geom_line(data = plt_data, aes(x = times, y =pexp(times, rate = lmda_hat, lower.tail=FALSE), color = \"Parametric\"),  linewidth=1.5) + \n  geom_step(data = ggdata, aes(x = time, y = estimate, color = 'KM'), linewidth=1.5) +\n  geom_line(aes(times, St, color = 'Ignore censoring' ), linewidth=1.5) + \n  labs(x = \"Time\", y = \"Survival probability\")+\n  scale_color_manual(name = \"Method\", values = c(\"Truth\" = 'green', 'Parametric'='orange', \"KM\" = 'blue', \"Ignore censoring\" = 'red'))\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Different statistical properties\n\nThere are different metrics that can help summarise a survival distribution for the purposes of comparing different distributions or reporting landmarks. \n\n### Median survival time\n\nIn the literature, the statistic we see most commonly is the _median survival time_. Due to censoring, we also have to be careful in estimating it, and need to use censoring-aware methods like the previous section, since it is derived from the estimated survival curve. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsfit <- survfit2(Surv(T, delta==\"Event\") ~ 1, data = d) # can also use survfit\nas_tibble(quantile(sfit, probs = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  quantile lower upper\n     <dbl> <dbl> <dbl>\n1    0.814 0.742 0.888\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggsurvfit(sfit)+\n  add_quantile(y_value = 0.5)\n```\n\n::: {.cell-output-display}\n![](week-05_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Mean survival time\n\nThe mean survival time seems an attractive metric; how long on average to subjects in the population survive? However, when we want to estimate mean survival time in the presence of censoring, we run into problems. In particular, if the survival curve doesn't touch the x-axis, which means that there remain some censored individuals at the end of followup, the mean survival time remains undefined, since theoretically the unknown survival times could be $\\infty$. \n\nWhat can be computed is the **restricted mean survival time (RMST)**, which asks what is the average survival time during a defined time period. It is \"computed\" as the area under the survival curve up to some specified time. We will use the {{< fa brands r-project >}} package **RISCA** to compute the RMST.[`install.packages('RISCA')`]{.aside}. This statistic is useful for comparing two survival curves, as we'll see in a bit. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(RISCA)\nsurv_data <- broom::tidy(sfit)\nwith(surv_data, rmst(time, estimate, max.time = 2, type = 's'))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9730756\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Comparing survival curves\n\n-   log-rank test\n\n-   \n\n\n",
    "supporting": [
      "week-05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}