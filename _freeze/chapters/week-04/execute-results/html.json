{
  "hash": "e0f0e93b2a2eddba4f2828d6a4675c37",
  "result": {
    "engine": "knitr",
    "markdown": "---\nengine: knitr\n---\n\n\n\n\n\n# Causal effect estimation {#sec-week4}\n\nLast updated: 19 Sep\\, 2024 11:16 AM EDT\n\n## Randomized studies\n\nSuppose we have the following causal graph, where we wish to ask the question of\nwhether the use of aspirin affects one's risk of a heart attack.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-04_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n\nIt is clear that there are several backdoor paths between aspirin and heart\nattack. However, if we randomize the assignment of aspirin in a randomized\nstudy, we break all paths to aspirin, and we can consider the following graph,\nwhich shows that only the causal path between aspirin and heart attack remains.\nIn the absence of confounders, the observed effect of aspirin on heart attacks\nin a randomized study is an estimate of the causal effect of aspirin on heart\nattacks.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](week-04_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe statistical field long held that the only way to assess causal effects was\nin the randomized study context, which became the gold standard for causal\neffect assessment. This has, in turn, propagated the use of randomized studies\nin economics and marketing, where it is often referred to as A/B testing.\n\nHowever, there are situations when randomized experiments are not feasible or\ncost prohibitive, or when we have observational data at hand. The question then\nis, can we analytically estimate causal effects without randomization.\n\n## Observational studies\n\nLet's start by trying to understand what randomized studies achieve. Primarily\nit is a question of breaking backdoor paths, which is done through balancing\npotential confounders between those exposed/treated and those not, *on average*.\nWe can then expect that those confounders no longer act as such since they are\nno longer empirically associated with the exposure, and the empirical effect of\nthe exposure on the outcome can be interpreted causally.\n\n**What if we can achieve similar balance in the confounders in an observational\nstudy?**\n\n### Weighting observations for statistical analysis\n\nWe first introduce the concept of weighting. Weighting is commonly used in\nanalyzing survey data to adjust for the way the survey sample is drawn, but the\nconcept is more general. The intent is to create a *pseudo-population* where the\nconfounders are balanced between levels of the exposure.\n\nLet's fix ideas with a toy example. In the following figure, we have 3 exposed\nand 4 non-exposed subjects, and the shading represents the values of a potential\nconfounder.\n\n![](images/tmp2.png){width=\"8in\"}\n\nThe central concept we introduce here is that of the **propensity score**.\nPropensity refers to the chance of being exposed, given what we know about\npotential confounders, so, symbolically,\n$P(\\text{exposed} | \\text{confounders})$ . In the above example, we have 1\nconfounder defined by the shading, and so we have\n\n-   P(exposed \\| black) = 1/3\n\n-   P(exposed \\| white) = 1/2\n\nThe scheme we now introduce is called *Inverse Probability of Treatment Weights\n(IPTW)*. We will weight each exposed unit by the inverse of the probability of\nbeing exposed given the confounder's value, and similarly we will weight each\nnon-exposed unit by the inverse of the probability of being not exposed. This\nresults in the following weights:\n\n![](images/tmp21.png)\n\nThe effect of the weights is effectively to clone the units, even fractionally.\nThis gets us to the following pseudo-population where the confounders are\nbalanced and so their effect in biasing the causal effect of interest is\nremoved.\n\n![](images/tmp.png){width=\"8in\"}\n\n## Propensity scores\n\nThe propensity score is defined as P(exposed \\| confounders). In the previous\nsection, we see how this works for a single binary confounder. Now we need to\nextend this concept to the situation where we have multiple confounders,\npossibly of different data types. The core idea is that we find an estimate of\nP(exposure \\| confounders) for all possible value combinations of confounder\nvalues, and then use the IPTW scheme to weight the units based on these scores.\n\nWe can address this problem using regression models or supervised learning\nmodels. For binary outcomes, we typically use **logistic regression**, but other\nsupervised learning choices like random forests, GBM, nearest neighbors, are\nalso usable. We note here that we are developing a model to estimate the\nprobability of being exposed based on *potential confounders*; this model does\nnot include the outcome at all.\n\n::: callout-note\nRegression models are basically meant to model E(Y\\|X), which is also called the\n*regression function* as a function of (potentially multivariate) X. In linear\nmodels we impose a particular functional form to estimate E(Y\\|X), but black box\nsupervised learning models attempt to do the same thing.\n\nWe note here that P(Y\\|X) = E(Y\\|X) when Y is binary (i.e. takes values 0 and\n1), and so estimating P(Y\\|X) can be seen as a regression problem as well.\n:::\n",
    "supporting": [
      "week-04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}