{
  "hash": "2f26578444c990efde093749f5c2c5ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to biological and biomedical data\"\nengine: knitr\n---\n\n\n\n\n\n**Last updated:** 04 Sep\\, 2024 08:51 PM EDT\n\n## History\n\nBiological data, in particular agricutural data, lay at the root of much of statistics and probability development in the 1800s and 1900s. In fact, biological understanding and statistical methodology have had a symbiotic relationship through the ages. Our understanding of genetic inheritance is basically statistical, and the field of design of experiments evolved to help make sense of whether particular seeds were better than other seeds. In more modern times, as biological methods and understanding have progressed, so have statistical methods to help understand the biology. Modern medicine also has benefitted from statistical advances, primarily in evaluating the safety and efficacy of drugs, devices and treatments addressing common and uncommon diseases, led by the efforts at the US Food and Drug Administration in the 1950s. The last century also saw massive statistical advances in understanding factors that affect the risk of disease and how to use high-throughput technologies to quickly evaluate huge numbers of molecular-level factors robustly, and identify anomalies in imaging modalities. The last 50 years especially have been a period of rapid change and improvement, but also a re-evaluation of how to best understand the biology from the data, and how to make decisions based on such data. \n\n## Biology\n\nStatistics has long had an influence in our understanding of biology. It has been central to the development of pest-resistant strains of grains, breeding of elite thoroughbred horses and productive livestock. As we'll see in the next section, much of this is based on our understanding of genetic inheritance. It also benefitted from the development of more efficient experimental designs so that we could efficiently identify whether some characteristics improved or not. The field of _design of experiments_ developed with agricultural experiments (famously, R.A. Fisher's work at the [Rothamsted Experimental Station](https://www.adelaide.edu.au/library/special/exhibitions/significant-life-fisher/rothamsted/) in England, and P. Mahalonobis' work with jute production in India). These developments then gave rise to study designs to efficiently investigate human disease (including work by Jerry Cornfield at the NIH), and the use of the _randomized controlled trial (RCT)_ as the study design providing rigorous evidence of causality and effectiveness of drugs and the basis for almost all drug approvals worldwide. \n\n::: callout-note\nMuch of the work in agriculture, biology and epidemiology translated into engineering and business quite quickly. \n\n- The ideas of experimental design and statistical quality control were used by Deming to improve industrial quality, mainly in Japan after his ideas were rejected by the American auto industry. This line of thought has extended and developed today into the Six Sigma principles and methods of quality assurance\n- Experimental design is central to engineering progress and standardization\n- The ideas of _A/B testing_ in business are basically _in silico_ versions of RCTs. Over the last 20 years, RCTs have become an accepted and central methodology in economics research\n\n:::\n\n## Genetics \n\n### Classical genetics\n\nOur understanding of genetic inheritance is based in the foundational experiments of Gregor Mendel, who showed in small agricultural experiments how particular traits in pea plants are transmitted from generation to generation. The basic ideas of [autosomal inheritance](https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/autosomal-dominant-inheritance#) and how many of each trait to _expect_ in each generation were established by statistical analyses, even though Mendel's experiments and theory pertained to a simplistic view of biology. This work was further extended and applied to many other fields by greats like [R.A. Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) and [J.B.S. Haldane](https://en.wikipedia.org/wiki/J._B._S._Haldane). \n\n::: callout-note\n### Did Mendel fabricate his data?\nThere has been controversy that Mendel's data was _too good_, and may have had an element of fabrication. [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) and R. A. Fisher both contributed statistical analyses to promote this, one of the first instances of **statistical forensics**. We'll see another more modern example later. More information about the controversy around Mendel's data can be learned [here](https://www.discovermagazine.com/the-sciences/did-gregor-mendel-fabricate-his-gene-data#). \n:::\n\nThe basic ideas started by Mendel was extended over time as our understanding of the complexities of genetic inheritance improved. We started understanding how inheritance worked biologically through mitosis and meiosis, and the role that _distance between genes_ plays in inheritance ([genetic linkage](https://en.wikipedia.org/wiki/Genetic_linkage)). This understanding led to understanding that Mendel's laws of _independent assortment_ have major exceptions, in that genes that are close together in the genome don't get inherited independently. This understanding also led to the design of [linkage studies](https://pubmed.ncbi.nlm.nih.gov/16168786/) and [linkage analysis](https://jamanetwork.com/journals/jamaneurology/fullarticle/775035#:~:text=Genetic%20linkage%20analysis%20is%20a,chromosome%20remain%20linked%20during%20meiosis.) to understand inheritable diseases like some cancers, diabetes and heart disease. This also led to a focus on studying families, siblings, and family trees to understand genetic factors of disease, and novel experimental designs for genetic association studies. \n\n::: callout-note\nPerhaps the most famous example of linkage analysis is in the prevalence of [hemophilia](https://www.mayoclinic.org/diseases-conditions/hemophilia/symptoms-causes/syc-20373327#:~:text=Overview,if%20your%20blood%20clotted%20properly.) among the royal families of Europe in the 1800s and 1900s; famously the hemophilia of the Russian Tsarevich Alexei and the rise of Rasputin entered Western culture through novels and movies. \n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Erbgang_Bluterkrankheit.svg/440px-Erbgang_Bluterkrankheit.svg.png)\n:::\n\n::: callout-warning\n### Intertwining statistics, genetics and eugenics in the early 1900s\nWe've already mentioned two stalwarts formative to the modern methodology and practice of statistics, R. A. Fisher and Karl Pearson, who were academic and professional rivals. Today, there is an strong realization that despite their seminal methodological contributions, their intentions were not ethical by today's standards. In particular, both were proponents of eugenics (see [Eugenics and Scientific Racism](https://www.genome.gov/about-genomics/fact-sheets/Eugenics-and-Scientific-Racism#:~:text=Eugenicists%20worldwide%20believed%20that%20they,by%20them%20to%20be%20unfit.) for an introduction), which was not uncommon in the early 1900s. Pearson, who developed the formalization of the correlation coefficient and the $\\chi^2$ test, and his mentor Francis Galton (who promoted the idea of \"regression to the mean\") were avowed eugenicists; Pearson founded the journals _Biometrika_ (which is still a prestigious journal) and the _Annals of Eugenics_, which transformed later to the _Annals of Human Genetics_, and was appointed the first _Galton Chair of Eugenics_ at the University of London. \n\nFisher was also interested in eugenics as it related to Mendelian inheritance (see [here](https://www.nature.com/articles/s41437-020-00394-6#Sec5)), and did progress from those ideas to develop the statistical basis for genetic inheritance. However, he did both talk and publish pro-eugenics views, and has led many instructors to stop using Fisher's iris data as a basic dataset for teaching statistics; it led to the rise of the [penguins](https://allisonhorst.github.io/palmerpenguins/).\n\n:::\n\n### Genomics\n\nA fundamental shift in biological technology arose in the 1990s with various high-throughput methods, with accompanying statistical methods. Foremost was the idea of a _microarray_, where we could interrogate multiple genes together across multiple samples. These were pioneered by [Patrick Brown](https://profiles.stanford.edu/patrick-brown) (@brown:1999) at Stanford University, and led to multiple advances in statistical methodology in the early 2000s at Stanford (Rob Tibshirani, Michael Eisen, David Botstein and others) and later at Johns Hopkins (Rafael Irizarry, among others). In particular, two seminal papers (@eisen:1998 and @tusher:2001) demonstrated the new statistical methodology needed to address the (then) new technology of cDNA microarrays and introduced unsupervised learning as a major data science methodology in biology. Later, we saw more detailed statistical analyses around quality assurance, normalization and proper statistical analyses of microarrays (@irizarry:2003).  \n\n::: callout-note\n### Microarrays and breast cancer\n\nOne of the early successes of microarray technology was in the subtyping of breast cancer (@sorlie:2001), which showed that breast cancer was not a homogeneous disease but had several distinct subtypes. This led, over the years, to a standard genetic panel that all breast cancer biopsies are subject to, looking at mutations in the estrogen receptor (ER), progesterone receptor (PR) and the _HER2_ gene. These now lead to 4 broad classes of breast cancer, each of which requires different treatment regimes. In particular the success of tamoxifen for ER+ cancer and heceptin for HER2+ cancer have made a majority of breast cancer treatable. \n\nMore information: [Breast cancer types: What your type means](https://www.mayoclinic.org/diseases-conditions/breast-cancer/in-depth/breast-cancer/art-20045654)\n\n:::\n\n### Other \"-omics\"\n\nIn the last 30 years, we have seen the growth of other high-throughput methods to look at a number of molecular properties, most prominently being _proteomics_, _metabolomics_, and _transcriptomics_. Sequencing of genes, i.e. learning the exact alphabet of the genome, become cheaper and more accurate due in part to efforts of scientists in the [Human Genome Project](https://www.genome.gov/human-genome-project), and today, next-generation sequencing (NGS) is widespread. \n\nStatistically, this lead to the development of _genome-wide association studies ([GWAS](https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies))_, which incorporated high-throughput genetic seqencing and [SNP](https://www.genome.gov/genetics-glossary/Single-Nucleotide-Polymorphisms) analysis to try and understand the genetic basis of disease. [Polygenic risk scores](https://www.genome.gov/Health/Genomics-and-Medicine/Polygenic-risk-scores) arising from high-throughput technologies have helped us understand how multiple genes can act together to increase the risk of disease. This led to several advances in understanding disease and potential treatment strategies. \n\nToday, our understanding of disease risk, disease etiology and the development of drug targets leverage high-throughput technologies, machine learning and artificial intelligence to develop new drugs, especially in oncology. \n\n### Software\n\nThis period was the beginning of the areas of _bioinformatics_ and _computational biology_, and germinated the growth of software, primarily in {{< fa brands r-project >}}, to address such problems in a collective known as the [Bioconductor project](https://www.bioconductor.org). This suite of software has become the industry standard for bioinformatic analysis and has made {{< fa brands r-project >}} the main analytic software used in this field. \n\n#### Looking forward\nWe will learn more about bioinformatics and making inferences and identifying biomarkers in @sec-week7 and @sec-week8\n\n## Understanding human disease\n\n### Collecting data\n\nThe earliest form of data-driven inference for human disease was based on observational data, i.e. data that was collected as it appeared and not as part of any organized structure or design. Observational data can be powerful in first showing **associations** between disease and causes and are often the first indications that something is going wrong. We have seen in DSAN 5300 the work of John Snow in identifying water wells that might have been responsible for a cholera outbreak in London in 1854. \n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Snow-cholera-map-1.jpg/600px-Snow-cholera-map-1.jpg)\n\nEpidemiologists and biostatisticians like [Jerome Cornfield](https://en.wikipedia.org/wiki/Jerome_Cornfield) started looking more carefully at study design, mainly in the context of smoking and lung cancer. The case-control design developed as a faster way of evaluating associations between disease and some factor.\n\nHowevever, observational studies cannot evaluate **causation** or establish a link between a factor and the development of a disease. This needs longitudinal cohort studies or randomized experiments, which became the gold standards for understanding disease, and in establishing interventions to prevent or cure disease. \n\n#### Looking forward\n\nWe'll discuss more about study design, levels of evidence and causality in @sec-week2, @sec-week3, and @sec-week4. We'll explore principles for planning studies in @sec-week10\n\n### How long do we live?\n\n![](../img/NSCLC.png)\n_Survival curves for overall survival in non-small cell lung cancer by the 8th edition of the TNM staging system (@yun:2019) _\n\nA central question in understanding disease is, how long do we live, and will a treatment let us live longer? This is a question of **survival**, and an entire ecosystem of methods and software developed to address this question. You have been introduced to the Kaplan-Meier curve and Cox regression, which are widely used, but are the tip of the iceberg. \n\nCox regression, especially, has been used within the context of RCTs to establish that individuals who get a new treatment live longer (have _lower risk of dying_) than individuals who were given a placebo or a current standard of care treatment. It established the role of the _hazard ratio_ as the crucial statistic to look at. The prominence of this method (which is about the same age as myself, @coxRegressionModelsLifeTables1972, but whose _bona fide_ as a good method for survival analysis was established a few years later. This is one of the most cited papers in all of statistics)\n\nThe central question in survival analysis is how do we handle incomplete information of a particular form. Everyone will eventually die, but since we cannot have infinite studies, we cannot observed all the deaths or when they happened. We also sometimes cannot observe the beginning of when someone was first exposed (to a carcinogen like asbestos, for example). All we know is that death occurred after some last observation time, or exposure happened before some first observation time. How do we account for this in our analyses?\n\n::: callout-tip\n## The curious want to know\n\n1. Why is accounting for this incomplete information important?\n2. What happens if we just ask, what fraction has survived 2 years, say? Is this more straightforward?\n3. What assumptions do we need to make to establish that a new treatment is indeed better than an old treatment? Or that a new treatment is no worse than an old treatment?\n:::\n\n#### Looking foward\nWe will explore survival analysis and how it is used for assessing treatments in @sec-week5 and @sec-week6.\n\n### Imaging as a source of biomarkers and evidence\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/MRI_of_Human_Brain.jpg/1119px-MRI_of_Human_Brain.jpg?20220615205518)\n\nMedical images are an essential piece of the diagnostic puzzle, be it x-rays, computerized tomography (CT), magnetic resonance images (MRI) or positron-emission tomography (PET). The rise of computational radiology and computational pathology have made the assessement of diagnostic medical images much easier, leveraging many machine learning techniques as well as domain knowledge. This area is getting stronger with the advance of computational tools, especially using deep learning tools.\n\n#### Looking forward\nWe will not explore this area much in this course.\n\n### Electronic health records as a source of data\n\nElectronic health records (EHR) have allowed the standardization and inter-operability of personal health records within and between healthcare organizations. They provide a textual and multimodal record of a patient's journey. As such, they can be analyzed using a variety of NLP techniques, and there is current interest in using large language models to provide predictions and reasoning for patient outcomes using EHRs\n\n#### Looking forward\n\nWe will defer these topics to the more specialized courses that include NLP and LLMs\n\n## The rest of the course\n\nOnce we understand the kinds of data and their particular characteristics that we can encounter in biological and biomedical areas, we will explore other methodologies that can help understand the inter-relationships between different factors, how modeling can help our understanding , and how we can predict outcomes. In particular, we will explore Bayesian models and reasoning, and how machine learning models can provide a more flexible method for looking at associations and causality. We will explore questions around explainable AI, and how all of this can help with making decisions around biological and clinical understanding and treatment choices. \n\n\n### \n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}