<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>12&nbsp; Machine learning explainability – Biological and Biomedical Data Science ([DSAN 6150](https://gu-dsan.github.io/6150-fall-2024/){target=_blank})</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/week-13.html" rel="next">
<link href="../chapters/week-11.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar docked nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Biological and Biomedical Data Science (</span></a><a href="https://gu-dsan.github.io/6150-fall-2024/" target="_blank">DSAN 6150</a>)
    
  </div>
        <div class="quarto-navbar-tools tools-end">
    <a href="https://github.com/araastat/6150-notes/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/week-12.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Machine learning explainability</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto"><div class="sidebar-tools-main">
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to biological and biomedical data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Experimental design and confounding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Causal effect estimation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Survival analysis I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Survival analysis II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Applied machine learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/week-12.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Machine learning explainability</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-technical-background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Technical background</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#attributing-risk-an-old-problem" id="toc-attributing-risk-an-old-problem" class="nav-link active" data-scroll-target="#attributing-risk-an-old-problem">Attributing risk: an old problem</a></li>
  <li>
<a href="#explainable-machine-learning" id="toc-explainable-machine-learning" class="nav-link" data-scroll-target="#explainable-machine-learning">Explainable machine learning</a>
  <ul class="collapse">
<li><a href="#feature-importance" id="toc-feature-importance" class="nav-link" data-scroll-target="#feature-importance">Feature importance</a></li>
  <li><a href="#shapley-additive-explanations-shap" id="toc-shapley-additive-explanations-shap" class="nav-link" data-scroll-target="#shapley-additive-explanations-shap">Shapley Additive Explanations (SHAP)</a></li>
  <li><a href="#local-interpretable-model-agnostic-explanations-lime" id="toc-local-interpretable-model-agnostic-explanations-lime" class="nav-link" data-scroll-target="#local-interpretable-model-agnostic-explanations-lime">Local Interpretable Model-agnostic Explanations (LIME)</a></li>
  </ul>
</li>
  <li><a href="#a-computational-example-using-the-boston-housing-data" id="toc-a-computational-example-using-the-boston-housing-data" class="nav-link" data-scroll-target="#a-computational-example-using-the-boston-housing-data">A computational example using the Boston Housing data</a></li>
  <li><a href="#g-computations-for-understanding-the-effect-of-a-predictor" id="toc-g-computations-for-understanding-the-effect-of-a-predictor" class="nav-link" data-scroll-target="#g-computations-for-understanding-the-effect-of-a-predictor">g-computations for understanding the effect of a predictor</a></li>
  <li>
<a href="#inherently-explainable-models" id="toc-inherently-explainable-models" class="nav-link" data-scroll-target="#inherently-explainable-models">Inherently explainable models</a>
  <ul class="collapse">
<li><a href="#causal-forests" id="toc-causal-forests" class="nav-link" data-scroll-target="#causal-forests">Causal forests</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/araastat/6150-notes/edit/main/chapters/week-12.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/araastat/6150-notes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-week12" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Machine learning explainability</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p><strong>Last updated:</strong> 13 Nov, 2024 10:50 AM EST</p>
<p>In the previous chapter we looked at machine learning models that can be used for survival data, from the point of view of predictions. This is of course useful, but in BDS we’re often more interested in understanding <strong>why</strong> something happens and use models to help understand that. In other words, we are more interested in <em>explanability</em> and <em>causality</em> when it comes to modeling. This interest is based on the needs for <strong>interventions</strong> to change responses (think treatments).</p>
<p>Parametric models, generally, are explainable insofar as each parameter we have in the model has a particular interpretation in terms of the conditional change in the repsonse if we change the corresponding predictor by 1 unit while keeping others constant. With the proper study design, that interpretation can be causal as well, rather than merely associative.</p>
<p>But what about “black-box” models? How can we make them more interpretable in terms of various inputs we might collect?</p>
<p>We will look at a few things here. First are ways to estimate explainable or attributable aspects from ML models via algorithms like LIME and Shapley values. Second is using causal <strong>g-computations</strong> to derive effects of features based on predictive models.</p>
<section id="attributing-risk-an-old-problem" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="attributing-risk-an-old-problem">Attributing risk: an old problem</h2>
<p>There is, in epidemiology, a concept called <strong>attributable risk</strong>, which is the portion of the disease risk or rate attributable to an exposure. In a randomized study, it can be measured as the difference in risk between the exposed and unexposed individuals (or, in other words, the causal effect). In observational studies, it is usually called the <em>population attributable risk (PAR)</em> and is defined to be difference in the overall incidence rate of the diesease and the incident rate among those who are unexposed, and can be interpreted as the reduction in risk if the exposure was removed from the population. This can be computed as</p>
<p><span class="math display">\[
AF = \frac{P(RR-1)}{P(RR-1)+1}
\]</span></p>
<p>where <span class="math inline">\(P\)</span> is the probability of exposure in the population, and <span class="math inline">\(RR\)</span> is the relative risk. This can be interpreted as based on context<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>:</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Benichou, J (2001). Stat Methods Med Res 10(3): 195-216</p></div></div><dl>
<dt>Cross-sectional study</dt>
<dd>
The AF represents the excess of prevalent cases of disease that can be attributed to an exposure
</dd>
<dt>Cohort study</dt>
<dd>
The AF represents how many excess cases of a disease we’d see that can be attributed to the exposure
</dd>
</dl></section><section id="explainable-machine-learning" class="level2"><h2 class="anchored" data-anchor-id="explainable-machine-learning">Explainable machine learning</h2>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Resources
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable by Christoph Molnar (<a href="https://christophm.github.io/interpretable-ml-book/">link</a>
</li>
<li>Explainable AI - Understanding and Trusting Machine Learning Models (<a href="https://www.datacamp.com/tutorial/explainable-ai-understanding-and-trusting-machine-learning-models">link</a>). This is a DataCamp tutorial on using SHAP and LIME.</li>
</ul>
</div>
</div>
<p>Explainable Machine Learning (XAI) refers to techniques and methods used to make the outputs of machine learning models understandable to humans. As machine learning models become increasingly complex, especially with the rise of deep learning and ensemble methods, the need for transparency and interpretability has grown. Traditional models like linear regression and decision trees are relatively easy to interpret, but more sophisticated models, such as neural networks and gradient boosting machines, are often seen as “black boxes” due to their intricate architectures and high dimensionality.</p>
<p>The primary goal of XAI is to bridge the gap between complex machine learning models and human interpretability. This involves providing insights into how models make decisions, identifying the factors that contribute to predictions, and ensuring that models operate fairly and without bias. Techniques for achieving explainability include feature importance measures, partial dependence plots, SHAP (SHapley Additive exPlanations) values, and LIME (Local Interpretable Model-agnostic Explanations). These methods help to decompose model predictions into understandable components, allowing stakeholders to trust and validate the results.</p>
<p>Explainable Machine Learning is crucial in areas where decisions have significant consequences, such as healthcare, finance, and law enforcement. For instance, in healthcare, understanding how a model predicts patient outcomes can inform better clinical decisions and identify potential risk factors. In finance, explainability ensures that credit scoring models do not discriminate against certain groups and comply with regulatory requirements. Moreover, explainable models foster accountability and transparency, making it easier to debug and improve them.</p>
<p>Ultimately, the pursuit of explainable machine learning aligns with the broader goals of responsible AI, which emphasizes fairness, accountability, and transparency. By making machine learning models more interpretable, we can enhance their reliability, foster user trust, and ensure that AI systems are used ethically and effectively.</p>
<section id="feature-importance" class="level3"><h3 class="anchored" data-anchor-id="feature-importance">Feature importance</h3>
<p>Variable feature importance is a common measure derived from machine learning models. What is most often computed is <strong>permutation feature importance</strong>, which asks how much predictive performance would be degraded if we replaced the predictor with a randomly permuted version of itself (which would, in effect, be non-informative). This is typically done one at a time to see the <strong>marginal</strong> feature importances averaged over values of all the other predictors</p>
<p>Feature importance scores can depend on the intrinsic variability of the predictor and the type of model used.</p>
</section><section id="shapley-additive-explanations-shap" class="level3"><h3 class="anchored" data-anchor-id="shapley-additive-explanations-shap">Shapley Additive Explanations (SHAP)</h3>
<p>SHAP (SHapley Additive exPlanations) is a powerful and widely used method for interpreting the outputs of complex machine learning models. It is grounded in cooperative game theory and provides a principled way to attribute the contribution of each feature to the final prediction of a model. To understand SHAP, imagine a scenario where various features of a dataset are treated as “players” in a game, and the prediction of a model is the “payout” or the outcome of that game. SHAP values help us determine how much each feature contributes to that outcome.</p>
<p>SHAP builds on the concept of Shapley values, named after Lloyd Shapley, who introduced them in 1953. Shapley values are used to fairly distribute the total gain (or payout) among all players based on their individual contributions. In the context of machine learning, SHAP values represent the average contribution of a feature to the prediction, considering all possible combinations of features. This ensures that the attribution of importance to each feature is both fair and consistent.</p>
<p>One of the key advantages of SHAP is its ability to provide both global and local interpretability. Global interpretability refers to understanding how the model behaves on average across the entire dataset, while local interpretability focuses on explaining individual predictions. SHAP achieves this by calculating the marginal contribution of each feature for every possible subset of features, which can then be aggregated to explain the model as a whole or a specific instance.</p>
<p>SHAP values are additive, meaning that the sum of the SHAP values for all features (plus a base value) equals the model’s prediction. This additivity property makes SHAP particularly intuitive and ensures that the contributions of features are directly comparable. Moreover, SHAP values are model-agnostic, meaning they can be applied to any machine learning model, whether it is a simple linear regression or a complex neural network.</p>
<p>To visualize SHAP values, various plots can be used, such as summary plots, dependence plots, and force plots. Summary plots provide a compact overview of feature importance and effects, dependence plots show the relationship between a feature and the prediction while accounting for interactions with other features, and force plots illustrate the contribution of each feature to a single prediction.</p>
<div class="callout callout-style-simple callout-tip">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>There are several implementations of SHAP in Python, to align with scikit-learn models. This is by far the richer ecosystem currently. We’ll provide some examples of R-based implmentations below and in the lab.</p>
</div>
</div>
</div>
</section><section id="local-interpretable-model-agnostic-explanations-lime" class="level3"><h3 class="anchored" data-anchor-id="local-interpretable-model-agnostic-explanations-lime">Local Interpretable Model-agnostic Explanations (LIME)</h3>
<p><strong>Understanding LIME (Local Interpretable Model-agnostic Explanations)</strong></p>
<p>LIME (Local Interpretable Model-agnostic Explanations) is a technique designed to explain the predictions of any machine learning model in an interpretable and comprehensive manner. The main idea behind LIME is to approximate the behavior of a complex model by using simpler, more interpretable models, but only in the local region around the prediction of interest. This approach allows us to understand why a model made a specific prediction without needing to fully decode the entire model.</p>
<p>Here’s how LIME works in a nutshell:</p>
<ol type="1">
<li><p><strong>Local Fidelity</strong>: LIME focuses on explaining individual predictions rather than the model as a whole. This is achieved by creating a local surrogate model, which is a simpler model that approximates the complex model in the vicinity of the specific instance being explained. The surrogate model is typically a linear model or a decision tree, which are easier to interpret.</p></li>
<li><p><strong>Perturbation and Sampling</strong>: To build the local surrogate model, LIME perturbs the data instance that needs to be explained by generating several variations of it. These perturbed instances are then fed into the complex model to obtain predictions. The idea is to understand how small changes in the input affect the prediction.</p></li>
<li><p><strong>Fitting the Surrogate Model</strong>: Once the perturbed instances and their corresponding predictions are obtained, LIME fits the local surrogate model to this new dataset. The surrogate model is trained to approximate the predictions of the complex model in the local region around the original instance.</p></li>
<li><p><strong>Interpreting the Explanation</strong>: The coefficients or structure of the surrogate model are then used to explain the prediction of the complex model. For example, in the case of a linear surrogate model, the coefficients indicate the importance and direction (positive or negative) of each feature in making the prediction.</p></li>
</ol>
<p>One of the key strengths of LIME is its model-agnostic nature. It can be applied to any type of machine learning model, whether it is a linear model, a decision tree, a support vector machine, or a deep neural network. This universality makes LIME a versatile tool for model interpretation.</p>
<p>Another advantage of LIME is its focus on local explanations. By concentrating on the local behavior of the model, LIME provides insights into specific predictions, which is particularly useful when dealing with instances that are critical or outliers. This local approach ensures that the explanations are both relevant and accurate for the instance being analyzed.</p>
<p>To visualize LIME explanations, various plots can be used. For instance, bar plots can show the contributions of different features to the prediction, making it easy to see which features are driving the model’s decision. This visual representation helps stakeholders, such as clinicians, financial analysts, or policy makers, understand and trust the model’s outputs.</p>
</section></section><section id="a-computational-example-using-the-boston-housing-data" class="level2"><h2 class="anchored" data-anchor-id="a-computational-example-using-the-boston-housing-data">A computational example using the Boston Housing data</h2>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Load necessary libraries</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://giuseppec.github.io/iml/">iml</a></span><span class="op">)</span> <span class="co"># interpretable machine learning</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Load the Boston dataset</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Boston"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Train a random forest model to predict median house value (medv)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># For reproducibility</span></span>
<span><span class="va">rf_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span><span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Boston</span>, ntree <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We create a <code>Predictor</code> object to hold the model and the data. This is a new R6 class object, so the syntax is a bit different</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Create a Predictor object from the iml package</span></span>
<span><span class="va">predictor</span> <span class="op">&lt;-</span> <span class="va"><a href="https://giuseppec.github.io/iml/reference/Predictor.html">Predictor</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">rf_model</span>, data <span class="op">=</span> <span class="va">Boston</span>, y <span class="op">=</span> <span class="va">Boston</span><span class="op">$</span><span class="va">medv</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can compute feature importance scores from this model</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">imp</span> <span class="op">&lt;-</span> <span class="va"><a href="https://giuseppec.github.io/iml/reference/FeatureImp.html">FeatureImp</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, loss <span class="op">=</span> <span class="st">'rmse'</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">imp</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="week-12_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can now run the SHAP algorithm on this predictor object</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Demonstrating SHAP values</span></span>
<span><span class="va">shapley</span> <span class="op">&lt;-</span> <span class="va"><a href="https://giuseppec.github.io/iml/reference/Shapley.html">Shapley</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, x.interest <span class="op">=</span> <span class="va">Boston</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="va">shapley</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="week-12_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Demonstrating LIME</span></span>
<span><span class="va">lime_explainer</span> <span class="op">&lt;-</span> <span class="va"><a href="https://giuseppec.github.io/iml/reference/LocalModel.html">LocalModel</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">predictor</span>, x.interest <span class="op">=</span> <span class="va">Boston</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span><span class="op">)</span></span>
<span><span class="va">lime_explainer</span><span class="op">$</span><span class="va">results</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             beta x.recoded     effect x.original feature feature.value
rm     0.44129009     6.575  2.9014824      6.575      rm      rm=6.575
lstat -0.04493674     4.980 -0.2237849       4.98   lstat    lstat=4.98
medv   0.80613895    24.000 19.3473349         24    medv       medv=24</code></pre>
</div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lime_explainer</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="week-12_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section><section id="g-computations-for-understanding-the-effect-of-a-predictor" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="g-computations-for-understanding-the-effect-of-a-predictor">g-computations for understanding the effect of a predictor</h2>
<p>G-computation, originally introduced by James Robins in 1986, is a statistical method used in causal inference to estimate the causal effect of a treatment or intervention on an outcome. It is particularly useful when dealing with complex observational data, where randomized controlled trials (RCTs) are not feasible. The “G” in g-computation stands for “generalized,” reflecting the method’s flexibility in handling various types of data and models.</p>
<p>The core idea of g-computation is to estimate the counterfactual outcomes – what would have happened to an individual if they had received a different treatment than they actually did. By comparing these counterfactual outcomes, we can infer the causal effect of the treatment. This is easiest to see when dealing with a binary variable representing treatment.</p>
<p>Here’s a step-by-step explanation of how g-computation works:</p>
<ol type="1">
<li><p><strong>Modeling the Outcome</strong>: First, we model the relationship between the treatment, covariates (confounders), and the outcome using a suitable statistical model, such as a regression model. This model should capture how the outcome depends on the treatment and other covariates.</p></li>
<li><p><strong>Prediction of Counterfactuals</strong>: Using the fitted model, we predict the counterfactual outcomes for each individual under both treatment scenarios – treated and untreated. This involves setting the treatment variable to each possible value (e.g., 0 for untreated and 1 for treated) while keeping the covariates fixed at their observed values.</p></li>
<li><p><strong>Averaging the Predictions</strong>: Once we have the counterfactual predictions for all individuals, we average these predictions to obtain the expected outcomes under each treatment scenario. This gives us the population-level estimates of the outcomes if everyone had received the treatment or if no one had received it.</p></li>
<li><p><strong>Estimating the Causal Effect</strong>: The causal effect of the treatment is then estimated by taking the difference (or ratio, depending on the context) between the expected outcomes under the treated and untreated scenarios. This difference represents the average treatment effect (ATE).</p></li>
</ol>
<p>Mathematically, the g-computation formula can be expressed as: <span class="math display">\[
\text{ATE} = \mathbb{E}[Y_{1}] - \mathbb{E}[Y_{0}]
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[Y_{1}]\)</span> is the expected outcome if everyone were treated, and <span class="math inline">\(\mathbb{E}[Y_{0}]\)</span> is the expected outcome if no one were treated.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>An alternative way of predicting the counterfactual, equally valid, was presented by Dasgupta, et al<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. Here we train two predictive models, one for the treated (<span class="math inline">\(M_T\)</span>) and one for the untreated (<span class="math inline">\(M_U\)</span>). Each captures the “landscape” for the respective groups. If we send the data from the untreated group down <span class="math inline">\(M_T\)</span>, we get the counterfactual prediction of what these individuals would experience if they had the same experience as the treated individuals. Similarly, we can send the data for the treated group down <span class="math inline">\(M_U\)</span>. So for each individual in the study, we have their actual experience and the predicted response if they had the experience of the other group. This gives an individual causal effect that can be averaged over the study population to get the <em>ATE</em>, as above, and you justify it using the same math as above.</p>
<p>Why do I like this more (other than the fact that I helped develop it)? I found that the strategy standardly proposed has the effect of breaking the inter-relationships between the predictors if one sets the predictor to particular values, resulting in unrealistic scenarios if the predictor has associations with other variables collected in the study.</p>
</div>
</div>
</div>
<div class="no-row-height column-margin column-container"><div id="fn2">
<ol type="1">
<li>Dasgupta, A., Szymczak, S., Moore, J. H., Bailey-Wilson, J. E. &amp; Malley, J. D. Risk estimation using probability machines. BioData Mining 7, 2 (2014).</li>
</ol>

</div></div><p>G-computation offers several advantages in causal inference:</p>
<ul>
<li>
<strong>Flexibility</strong>: It can handle various types of data, including continuous, binary, and time-to-event outcomes.</li>
<li>
<strong>Robustness</strong>: By explicitly modeling the relationship between treatment, covariates, and outcome, g-computation can adjust for confounding variables and provide unbiased estimates of causal effects.</li>
<li>
<strong>Counterfactual Reasoning</strong>: It allows for the estimation of counterfactual outcomes, which are crucial for causal inference.</li>
</ul>
<p>However, g-computation also has some limitations:</p>
<ul>
<li>
<strong>Model Dependence</strong>: The accuracy of g-computation relies on the correct specification of the outcome model. Misspecification of the model can lead to biased estimates.</li>
<li>
<strong>Computational Complexity</strong>: For large datasets or complex models, or to estimate higher-order effects, g-computation can be computationally intensive.</li>
</ul></section><section id="inherently-explainable-models" class="level2"><h2 class="anchored" data-anchor-id="inherently-explainable-models">Inherently explainable models</h2>
<section id="causal-forests" class="level3"><h3 class="anchored" data-anchor-id="causal-forests">Causal forests</h3>
<p>Causal forests are a machine learning technique designed to estimate heterogeneous treatment effects, which means they can determine how the effect of a treatment or intervention varies across different subgroups within a population. This is particularly useful in fields such as healthcare, economics, and social sciences, where the impact of an intervention may not be uniform across all individuals.</p>
<p>Causal forests extend the concept of random forests, a popular ensemble learning method for classification and regression, to the realm of causal inference. They were introduced as part of the broader framework of generalized random forests by Susan Athey and colleagues.</p>
<p>Causal forests are developed as follows:</p>
<ol start="2" type="1">
<li><p><strong>Adapting to Causal Inference</strong>: Causal forests are ensembles of decision trees, much like random forests. Unlike traditional random forests that focus on prediction accuracy, causal forests are designed to estimate treatment effects. The goal is to partition the data in a way that allows us to compare treated and untreated units within each partition to estimate the local treatment effect.</p></li>
<li><p><strong>Splitting Criterion</strong>: In causal forests, the trees are grown using a splitting criterion that aims to maximize the heterogeneity of treatment effects within the resulting partitions. This involves selecting splits that create subgroups where the difference in outcomes between treated and untreated units is most pronounced.</p></li>
<li><p><strong>Honest Estimation</strong>: Causal forests employ a technique called “honest estimation” to avoid overfitting and ensure unbiased estimates of treatment effects. This involves splitting the data into two subsets: one for creating the tree structure (splitting) and another for estimating the treatment effects within the leaves. This separation helps to prevent the model from tailoring the splits to the specific outcomes of the training data.</p></li>
<li><p><strong>Estimating Treatment Effects</strong>: Once the trees are grown, the causal forest aggregates the estimates from all trees to obtain a robust estimate of the treatment effect for each individual. The final treatment effect estimate for an individual is typically an average of the treatment effects estimated in the leaves of the trees where the individual ends up.</p></li>
<li><p><strong>Interpreting Results</strong>: Causal forests provide a way to understand how treatment effects vary across different subgroups. For example, in a medical study, a causal forest might reveal that a new drug is more effective for younger patients than for older ones. This can be visualized through variable importance plots or partial dependence plots that show how treatment effects change with different covariates.</p></li>
</ol>
<p><strong>Advantages of Causal Forests:</strong></p>
<ul>
<li>
<strong>Nonparametric Flexibility</strong>: They do not assume a specific functional form for the treatment effect, making them flexible and capable of capturing complex relationships.</li>
<li>
<strong>Heterogeneity Detection</strong>: They are well-suited for identifying and estimating heterogeneous treatment effects, providing insights into how different subgroups respond to an intervention.</li>
<li>
<strong>Scalability</strong>: Like random forests, causal forests can handle large datasets efficiently.</li>
</ul>
<p><strong>Limitations of Causal Forests:</strong></p>
<ul>
<li>
<strong>Complexity</strong>: They can be computationally intensive, especially with large datasets and many features.</li>
<li>
<strong>Interpretability</strong>: While they provide estimates of treatment effects, the underlying model can be complex and harder to interpret than simpler linear models.</li>
</ul>
<p>In summary, causal forests are a powerful tool for causal inference that leverage the strengths of random forests to estimate heterogeneous treatment effects. They provide valuable insights into how treatments impact different subgroups, making them an essential tool in personalized medicine, policy evaluation, and other areas where understanding individual-level treatment effects is crucial.</p>


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.araastat\.com\/6150-notes");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../chapters/week-11.html" class="pagination-link" aria-label="Applied machine learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Applied machine learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/week-13.html" class="pagination-link" aria-label="Decision making">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Decision making</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright Abhijit Dasgupta (2024). All rights reserved</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/araastat/6150-notes/edit/main/chapters/week-12.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/araastat/6150-notes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org">Quarto</a></p>
</div>
  </div>
</footer>


</body></html>