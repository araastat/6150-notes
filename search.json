[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biological and Biomedical Data Science (DSAN 6150)",
    "section": "",
    "text": "Introduction\nWelcome to Biological and Biomedical Data Science, a second-year course in the Georgetown University Masters in Data Analytics program.\nBiological and biomedical data are at the heart of many industries, including the life sciences, healthcare, pharmaceuticals and biotechnology, to name a few. Research and development in these industries aim to create new knowledge about biology (human, animal or agricultural) and use that knowledge to create products (medicines, vaccines, seeds, pesticides) that can help their target audiences. Data is central to these purposes, but unfortunately, like most things, it’s not quite so simple.\nIn data science, we have become used to situations where you have massive amounts of data that are available to you for mining and exploring, typically from tech giants, cloud providers and fintech. Data in the life sciences is much more limited; a drug can be approved with as few as 1000 patients in a clinical trial. Data is both harder and more expensive to collect, and so we are often much more careful about how we collect and utilize data. For context, a laboratory experiment may consist of 10 mice, and initial clinical studies might include 15 subjects. Experiments are costly, and so are often limited.\nThe purpose to which we use the data is also quite different. Most data science applications you may encounter revolve around predictive modeling, where the intent is to create robust predictions even if the model is a black box; we don’t really care how the predictions are generated but rather whether the predictions are accurate or robust. In the life sciences, we’re often more interested in what factors influence the prediction, why we see the patterns we see, and can the patterns be attributable to a set of factors either observable or imposed. So the currency we deal with involves correlations, associations and causality. We’re interested to get an understanding of how Nature works.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-science-topics",
    "href": "index.html#data-science-topics",
    "title": "Biological and Biomedical Data Science (DSAN 6150)",
    "section": "Data science topics",
    "text": "Data science topics\nBiological and biomedical data poses several unique features that need to be addressed in data science, sometimes by specialized methods. Some of the topics that we will cover in this course include\n\nCollecting data, and levels of evidence from how the data is collected\n\nObservational studies, epidemioligical studies, randomized studies\nConfounding and causality, direct and indirect effects\nAccommodating imperfect study design to make better inferences and decisions\nThe role of randomization and stratification in making better inference\n\nMissing data, particularly censoring and truncation, and how to accommodate them to make valid estimation and inference\nHigh-dimensional data (collecting lots of information on relatively few units of observation)\n\nInformation content\nML methods help (penalized models, gradient boosting, trees)\nReproducibility can suffer due to non-specificity of particular effects\n\nType I and II error, and how multiple testing affects it\np-values as a necessary but poor decision tool, and how we can do better\nBayesian approaches to estimation, inference and decision-making\nAnd the list can go on …",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "chapters/week-01.html",
    "href": "chapters/week-01.html",
    "title": "1  Introduction to biological and biomedical data",
    "section": "",
    "text": "History\nBiological data, in particular agricutural data, lay at the root of much of statistics and probability development in the 1800s and 1900s. In fact, biological understanding and statistical methodology have had a symbiotic relationship through the ages. Our understanding of genetic inheritance is basically statistical, and the field of design of experiments evolved to help make sense of whether particular seeds were better than other seeds. In more modern times, as biological methods and understanding have progressed, so have statistical methods to help understand the biology. Modern medicine also has benefitted from statistical advances, primarily in evaluating the safety and efficacy of drugs, devices and treatments addressing common and uncommon diseases, led by the efforts at the US Food and Drug Administration in the 1950s. The last century also saw massive statistical advances in understanding factors that affect the risk of disease and how to use high-throughput technologies to quickly evaluate huge numbers of molecular-level factors robustly, and identify anomalies in imaging modalities. The last 50 years especially have been a period of rapid change and improvement, but also a re-evaluation of how to best understand the biology from the data, and how to make decisions based on such data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biological and biomedical data</span>"
    ]
  },
  {
    "objectID": "chapters/week-01.html#biology",
    "href": "chapters/week-01.html#biology",
    "title": "1  Introduction to biological and biomedical data",
    "section": "Biology",
    "text": "Biology\nStatistics has long had an influence in our understanding of biology. It has been central to the development of pest-resistant strains of grains, breeding of elite thoroughbred horses and productive livestock. As we’ll see in the next section, much of this is based on our understanding of genetic inheritance. It also benefitted from the development of more efficient experimental designs so that we could efficiently identify whether some characteristics improved or not. The field of design of experiments developed with agricultural experiments (famously, R.A. Fisher’s work at the Rothamsted Experimental Station in England, and P. Mahalonobis’ work with jute production in India). These developments then gave rise to study designs to efficiently investigate human disease (including work by Jerry Cornfield at the NIH), and the use of the randomized controlled trial (RCT) as the study design providing rigorous evidence of causality and effectiveness of drugs and the basis for almost all drug approvals worldwide.\n\n\n\n\n\n\nMuch of the work in agriculture, biology and epidemiology translated into engineering and business quite quickly.\n\nThe ideas of experimental design and statistical quality control were used by Deming to improve industrial quality, mainly in Japan after his ideas were rejected by the American auto industry. This line of thought has extended and developed today into the Six Sigma principles and methods of quality assurance\nExperimental design is central to engineering progress and standardization\nThe ideas of A/B testing in business are basically in silico versions of RCTs. Over the last 20 years, RCTs have become an accepted and central methodology in economics research",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biological and biomedical data</span>"
    ]
  },
  {
    "objectID": "chapters/week-01.html#genetics",
    "href": "chapters/week-01.html#genetics",
    "title": "1  Introduction to biological and biomedical data",
    "section": "Genetics",
    "text": "Genetics\n\nClassical genetics\nOur understanding of genetic inheritance is based in the foundational experiments of Gregor Mendel, who showed in small agricultural experiments how particular traits in pea plants are transmitted from generation to generation. The basic ideas of autosomal inheritance and how many of each trait to expect in each generation were established by statistical analyses, even though Mendel’s experiments and theory pertained to a simplistic view of biology. This work was further extended and applied to many other fields by greats like R.A. Fisher and J.B.S. Haldane.\n\n\n\n\n\n\nDid Mendel fabricate his data?\n\n\n\nThere has been controversy that Mendel’s data was too good, and may have had an element of fabrication. Karl Pearson and R. A. Fisher both contributed statistical analyses to promote this, one of the first instances of statistical forensics. We’ll see another more modern example later. More information about the controversy around Mendel’s data can be learned here.\n\n\nThe basic ideas started by Mendel was extended over time as our understanding of the complexities of genetic inheritance improved. We started understanding how inheritance worked biologically through mitosis and meiosis, and the role that distance between genes plays in inheritance (genetic linkage). This understanding led to understanding that Mendel’s laws of independent assortment have major exceptions, in that genes that are close together in the genome don’t get inherited independently. This understanding also led to the design of linkage studies and linkage analysis to understand inheritable diseases like some cancers, diabetes and heart disease. This also led to a focus on studying families, siblings, and family trees to understand genetic factors of disease, and novel experimental designs for genetic association studies.\n\n\n\n\n\n\nPerhaps the most famous example of linkage analysis is in the prevalence of hemophilia among the royal families of Europe in the 1800s and 1900s; famously the hemophilia of the Russian Tsarevich Alexei and the rise of Rasputin entered Western culture through novels and movies.\n\n\n\n\n\n\n\n\n\n\nIntertwining statistics, genetics and eugenics in the early 1900s\n\n\n\nWe’ve already mentioned two stalwarts formative to the modern methodology and practice of statistics, R. A. Fisher and Karl Pearson, who were academic and professional rivals. Today, there is an strong realization that despite their seminal methodological contributions, their intentions were not ethical by today’s standards. In particular, both were proponents of eugenics (see Eugenics and Scientific Racism for an introduction), which was not uncommon in the early 1900s. Pearson, who developed the formalization of the correlation coefficient and the \\(\\chi^2\\) test, and his mentor Francis Galton (who promoted the idea of “regression to the mean”) were avowed eugenicists; Pearson founded the journals Biometrika (which is still a prestigious journal) and the Annals of Eugenics, which transformed later to the Annals of Human Genetics, and was appointed the first Galton Chair of Eugenics at the University of London.\nFisher was also interested in eugenics as it related to Mendelian inheritance (see here), and did progress from those ideas to develop the statistical basis for genetic inheritance. However, he did both talk and publish pro-eugenics views, and has led many instructors to stop using Fisher’s iris data as a basic dataset for teaching statistics; it led to the rise of the penguins.\n\n\n\n\nGenomics\nA fundamental shift in biological technology arose in the 1990s with various high-throughput methods, with accompanying statistical methods. Foremost was the idea of a microarray, where we could interrogate multiple genes together across multiple samples. These were pioneered by Patrick Brown (Brown and Botstein (1999)) at Stanford University, and led to multiple advances in statistical methodology in the early 2000s at Stanford (Rob Tibshirani, Michael Eisen, David Botstein and others) and later at Johns Hopkins (Rafael Irizarry, among others). In particular, two seminal papers (Eisen et al. (1998) and Tusher, Tibshirani, and Chu (2001)) demonstrated the new statistical methodology needed to address the (then) new technology of cDNA microarrays and introduced unsupervised learning as a major data science methodology in biology. Later, we saw more detailed statistical analyses around quality assurance, normalization and proper statistical analyses of microarrays (Irizarry et al. (2003)).\n\n\n\n\n\n\nMicroarrays and breast cancer\n\n\n\nOne of the early successes of microarray technology was in the subtyping of breast cancer (Sørlie et al. (2001)), which showed that breast cancer was not a homogeneous disease but had several distinct subtypes. This led, over the years, to a standard genetic panel that all breast cancer biopsies are subject to, looking at mutations in the estrogen receptor (ER), progesterone receptor (PR) and the HER2 gene. These now lead to 4 broad classes of breast cancer, each of which requires different treatment regimes. In particular the success of tamoxifen for ER+ cancer and heceptin for HER2+ cancer have made a majority of breast cancer treatable.\nMore information: Breast cancer types: What your type means\n\n\n\n\nOther “-omics”\nIn the last 30 years, we have seen the growth of other high-throughput methods to look at a number of molecular properties, most prominently being proteomics, metabolomics, and transcriptomics. Sequencing of genes, i.e. learning the exact alphabet of the genome, become cheaper and more accurate due in part to efforts of scientists in the Human Genome Project, and today, next-generation sequencing (NGS) is widespread.\nStatistically, this lead to the development of genome-wide association studies (GWAS), which incorporated high-throughput genetic seqencing and SNP analysis to try and understand the genetic basis of disease. Polygenic risk scores arising from high-throughput technologies have helped us understand how multiple genes can act together to increase the risk of disease. This led to several advances in understanding disease and potential treatment strategies.\nToday, our understanding of disease risk, disease etiology and the development of drug targets leverage high-throughput technologies, machine learning and artificial intelligence to develop new drugs, especially in oncology.\n\n\nSoftware\nThis period was the beginning of the areas of bioinformatics and computational biology, and germinated the growth of software, primarily in , to address such problems in a collective known as the Bioconductor project. This suite of software has become the industry standard for bioinformatic analysis and has made  the main analytic software used in this field.\n\nLooking forward\nWe will learn more about bioinformatics and making inferences and identifying biomarkers in Chapter 7 and Chapter 8",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biological and biomedical data</span>"
    ]
  },
  {
    "objectID": "chapters/week-01.html#understanding-human-disease",
    "href": "chapters/week-01.html#understanding-human-disease",
    "title": "1  Introduction to biological and biomedical data",
    "section": "Understanding human disease",
    "text": "Understanding human disease\n\nCollecting data\nThe earliest form of data-driven inference for human disease was based on observational data, i.e. data that was collected as it appeared and not as part of any organized structure or design. Observational data can be powerful in first showing associations between disease and causes and are often the first indications that something is going wrong. We have seen in DSAN 5300 the work of John Snow in identifying water wells that might have been responsible for a cholera outbreak in London in 1854.\n\nEpidemiologists and biostatisticians like Jerome Cornfield started looking more carefully at study design, mainly in the context of smoking and lung cancer. The case-control design developed as a faster way of evaluating associations between disease and some factor.\nHowevever, observational studies cannot evaluate causation or establish a link between a factor and the development of a disease. This needs longitudinal cohort studies or randomized experiments, which became the gold standards for understanding disease, and in establishing interventions to prevent or cure disease.\n\nLooking forward\nWe’ll discuss more about study design, levels of evidence and causality in Chapter 2, Chapter 3, and Chapter 4. We’ll explore principles for planning studies in Chapter 10\n\n\n\nHow long do we live?\n Survival curves for overall survival in non-small cell lung cancer by the 8th edition of the TNM staging system (Yun et al. (2019)) \nA central question in understanding disease is, how long do we live, and will a treatment let us live longer? This is a question of survival, and an entire ecosystem of methods and software developed to address this question. You have been introduced to the Kaplan-Meier curve and Cox regression, which are widely used, but are the tip of the iceberg.\nCox regression, especially, has been used within the context of RCTs to establish that individuals who get a new treatment live longer (have lower risk of dying) than individuals who were given a placebo or a current standard of care treatment. It established the role of the hazard ratio as the crucial statistic to look at. The prominence of this method (which is about the same age as myself, Cox (1972), but whose bona fide as a good method for survival analysis was established a few years later. This is one of the most cited papers in all of statistics)\nThe central question in survival analysis is how do we handle incomplete information of a particular form. Everyone will eventually die, but since we cannot have infinite studies, we cannot observed all the deaths or when they happened. We also sometimes cannot observe the beginning of when someone was first exposed (to a carcinogen like asbestos, for example). All we know is that death occurred after some last observation time, or exposure happened before some first observation time. How do we account for this in our analyses?\n\n\n\n\n\n\nThe curious want to know\n\n\n\n\nWhy is accounting for this incomplete information important?\nWhat happens if we just ask, what fraction has survived 2 years, say? Is this more straightforward?\nWhat assumptions do we need to make to establish that a new treatment is indeed better than an old treatment? Or that a new treatment is no worse than an old treatment?\n\n\n\n\nLooking foward\nWe will explore survival analysis and how it is used for assessing treatments in Chapter 5 and Chapter 6.\n\n\n\nImaging as a source of biomarkers and evidence\n\nMedical images are an essential piece of the diagnostic puzzle, be it x-rays, computerized tomography (CT), magnetic resonance images (MRI) or positron-emission tomography (PET). The rise of computational radiology and computational pathology have made the assessement of diagnostic medical images much easier, leveraging many machine learning techniques as well as domain knowledge. This area is getting stronger with the advance of computational tools, especially using deep learning tools.\n\nLooking forward\nWe will not explore this area much in this course.\n\n\n\nElectronic health records as a source of data\nElectronic health records (EHR) have allowed the standardization and inter-operability of personal health records within and between healthcare organizations. They provide a textual and multimodal record of a patient’s journey. As such, they can be analyzed using a variety of NLP techniques, and there is current interest in using large language models to provide predictions and reasoning for patient outcomes using EHRs\n\nLooking forward\nWe will defer these topics to the more specialized courses that include NLP and LLMs",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biological and biomedical data</span>"
    ]
  },
  {
    "objectID": "chapters/week-01.html#the-rest-of-the-course",
    "href": "chapters/week-01.html#the-rest-of-the-course",
    "title": "1  Introduction to biological and biomedical data",
    "section": "The rest of the course",
    "text": "The rest of the course\nOnce we understand the kinds of data and their particular characteristics that we can encounter in biological and biomedical areas, we will explore other methodologies that can help understand the inter-relationships between different factors, how modeling can help our understanding , and how we can predict outcomes. In particular, we will explore Bayesian models and reasoning, and how machine learning models can provide a more flexible method for looking at associations and causality. We will explore questions around explainable AI, and how all of this can help with making decisions around biological and clinical understanding and treatment choices.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biological and biomedical data</span>"
    ]
  },
  {
    "objectID": "chapters/week-01.html#references",
    "href": "chapters/week-01.html#references",
    "title": "1  Introduction to biological and biomedical data",
    "section": "References",
    "text": "References\n\n\n\n\nBrown, P. O., and D. Botstein. 1999. “Exploring the new world of the genome with DNA microarrays.” Nature Genetics 21 (1 Suppl): 33–37. https://doi.org/10.1038/4462.\n\n\nCox, D. R. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (2): 187–202. https://doi.org/10.1111/j.2517-6161.1972.tb00899.x.\n\n\nEisen, M. B., P. T. Spellman, P. O. Brown, and D. Botstein. 1998. “Cluster analysis and display of genome-wide expression patterns.” Proceedings of the National Academy of Sciences of the United States of America 95 (25): 14863–68. https://doi.org/10.1073/pnas.95.25.14863.\n\n\nIrizarry, Rafael A., Bridget Hobbs, Francois Collin, Yasmin D. Beazer-Barclay, Kristen J. Antonellis, Uwe Scherf, and Terence P. Speed. 2003. “Exploration, normalization, and summaries of high density oligonucleotide array probe level data.” Biostatistics (Oxford, England) 4 (2): 249–64. https://doi.org/10.1093/biostatistics/4.2.249.\n\n\nSørlie, Therese, Charles M. Perou, Robert Tibshirani, Turid Aas, Stephanie Geisler, Hilde Johnsen, Trevor Hastie, et al. 2001. “Gene Expression Patterns of Breast Carcinomas Distinguish Tumor Subclasses with Clinical Implications.” Proceedings of the National Academy of Sciences 98 (19): 10869–74. https://doi.org/10.1073/pnas.191367098.\n\n\nTusher, Virginia Goss, Robert Tibshirani, and Gilbert Chu. 2001. “Significance Analysis of Microarrays Applied to the Ionizing Radiation Response.” Proceedings of the National Academy of Sciences 98 (9): 5116–21. https://doi.org/10.1073/pnas.091062498.\n\n\nYun, Jae Kwang, Geun Dong Lee, Hyeong Ryul Kim, Yong-Hee Kim, Dong Kwan Kim, Seung-Il Park, and Sehoon Choi. 2019. “Validation of the 8th Edition of the TNM Staging System in 3,950 Patients with Surgically Resected Non-Small Cell Lung Cancer.” Journal of Thoracic Disease 11 (7): 2955–64. https://doi.org/10.21037/jtd.2019.07.43.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to biological and biomedical data</span>"
    ]
  },
  {
    "objectID": "chapters/week-02.html",
    "href": "chapters/week-02.html",
    "title": "2  Experimental design and confounding",
    "section": "",
    "text": "Introduction\nIn data science, we spend most of our time learning about how to learn from data, and developing models to do just that. We can always develop a model that learns from a dataset, but the way that the data is collected can affect the external validity of the model, i.e., how well the model can generalize to other data sets. We have a innate idea that a good training dataset is, in some ways, representative of the population we want to run future predictions, in order to have a useful predictive model.\nThe way we collect the data can be important, and can certainly have downstream implications in how we analyze data and how we interpret our results. This is the main reason for design of experiments.\nCareful planned collection is much more important if the data is expensive to collect, and we can only collect smaller amounts of data. We have to make the most of the data we have for making our analytic results as strong as possible. In the life sciences and in biomedical contexts, this is the general situation.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Experimental design and confounding</span>"
    ]
  },
  {
    "objectID": "chapters/week-02.html#introduction",
    "href": "chapters/week-02.html#introduction",
    "title": "2  Experimental design and confounding",
    "section": "",
    "text": "Clinical trials are expensive!!\n\n\n\nPhase III clinical trials are funded by pharmaceutical companies to provide definitive evidence of a drug’s effectiveness. A phase III trial can cost multiple billions of dollars, for testing the drug on perhaps several hundred subjects. Each participant can be subject to multiple expensive tests, costs of transport to clinical centers, treatments to help with side effects and unwanted problems, and other kinds of expenses.\nGiven the costs per patient, pharmaceutical companies have a strong interest in getting the strongest results possible from the fewest numbers of participants, and a lot of effort goes into designing the trial so that the “treatment effect” can be robustly ascertained, since this is the basis of drug approvals. The trial design goes through multiple iterations within the company, and then needs agreeement from the regulatory agencies like the FDA and EMA, before the trial can go to the field and the first subject enrolled.\nWith billions of dollars on the line, finding the right patients and the right study design are imperative",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Experimental design and confounding</span>"
    ]
  },
  {
    "objectID": "chapters/week-02.html#good-study-design",
    "href": "chapters/week-02.html#good-study-design",
    "title": "2  Experimental design and confounding",
    "section": "Good study design",
    "text": "Good study design\nMost studies are done to to see if an intervention of interest has an effect on some result of interest.\nSome Examples\n\nSeeing if a new fertilizer can help improve yields of wheat (link)\nSeeing if a non-narcotic drug can help reduce pain post-surgery (link)\nSeeing if a particular advertisement can improve sales of a product (link)\nSeeing if a GLP-1 inhibitor can help reduce a person’s weight (link)\nHow to collect data to get robust results?\nThere are several questions we might want to think about when we collect data for finding a result (fairly!!).\n\nIs the data we’re looking at representative of the population that we are interested in? \n\nCan we make fair comparisons within the data?\nCan we see robust results using less data?\nIs the data we’re using biased in any way?\n\nRemind ourselves of the concepts of sample and population\n\n\n\n\n\nFood for thought\n\n\n\nWhat other questions might we address that might guide our data collection?\n\n\nBiases\nThe main point here is fairness. We want results that are not subject to intentional and unintentional biases. There are several kinds of biases that we need to be careful of, that we’ll highlight throughout this course. Some of the more common biases are: Think about why these biases can appear, and how they can be minimized. Also think about other biases that might arise.\n\n\nObserver bias which can arise when there are systematic differences between true and recorded values, or this difference depends upon who is collecting the data. This can happen if, for example, a doctor knows the treatment a patient received. Another example might be in how a doctor measures pain levels depending on the race or ethnicity of a patient.\n\nSelection bias which can arise when how subjects are selected can affect the final inference. For example, if you assign the first 100 patients who come to a hospital on a Friday to one group and a second 100 patients who come to the hospital on Monday to another group, there may be intrinsic differences that may explain any differences you might see that are not related to treatment.\n\nConfirmation bias, the tendency to interpret information to support a pre-existing idea or notion\n\nResponse bias which occurs when the answers given by a patient are influenced by extraneous factors\n\nNon-response bias which is seen when there are differences in characteristics of individuals who respond and don’t respond to a survey. This is particularly evident in ratings of products available at an online retailer, for example.\n\nRecall bias which occurs in retrospective studies where subjects recall their experience depending on their condition. This issue is rampant in nutritional epidemiology where subjects are asked to recall everything they ate in the last week (a 7-day food frequency questionnaire). A simpler example might be where people diagnosed with lung cancer overestimate their tobacco use while healthy people underestimate it.\n\n\n\n\n\n\n\nA historical blunder: “Dewey defeats Truman”\n\n\n\n\n\n\n\nAs the 1948 US Presidential election between Harry Truman, Thomas Dewey and Strom Thurmond unfolded, the Chicago Tribune published the now-infamous headline “Dewey defeats Truman” late on election night. As it turned out, Truman won in a landslide.\nThis was a result of two kinds of biases – sampling (selection) bias and confirmation bias.\nThe surveys done to elicit voter responses were done via telephone, which is an efficient way to survey people. Unfortunately, in 1948, telephones were a bit of a luxury and tended to be in wealthier households, who skewed towards Dewey. So the data collected was not representative. Also, surveys were done 2 weeks before the election and so opinions, and votes, changed.\nThere was also confirmation bias, in that conventional wisdom also was that Dewey would win, and so the writer and analyst Arthur Henning, who penned the article and the headline, stuck to his belief that Dewey would win, and early returns from Republican strongholds appeared to confirm that belief.\nEven though the 1948 story was learned from, and polling agencies like Gallup learned to avoid the obvious biases, the 2016 elections brought polling back into the limelight. Post-hoc analyses did consider whether there were sampling biases, but generally it was concluded that the results were within the polls’ margins of error, in that polling results are not deterministic but may be off by a certain fraction. A really nice analysis of the 2020 polling results and the potential for sampling biases was done in The Scientific American.\n\n\n\n\nOther examples os sampling bias are here\nPrinciples\nThe questions around how to best collect data have generated significant statistical thought, with new contexts giving rise to newer methodologies. As we’ve talked about, a lot of statistical thought was generated from agricultural experiments. R.A. Fisher wrote two of the most influential texts on how to approach these topics in the context of his work at the Rothamstead research facility:\n\nStatistical Methods for Research Workers (1925) [link]\nDesign of Experiments (1935) [link]\n\nFisher’s work, and work by others through the early twentieth century, helped develop the following basic principles of experiemental design that allow a researcher to conclude that the results that are seen can be attributed to the treatments under study. They help avoid systematic errors and minimize random errors in order to make strong inferences about the treatment.\n\n\nWe will be looking into how we can “attribute” results to a cause next week.\n\n\n\n\n\n\nPrinciples of experimental design\n\n\n\n\nRandomization\n\nRandomly assign subjects to groups to minimize selection bias. The act of randomization creates a natural balance between groups on average and should minimize biases and confounding (add link )\n\nLocal Control\n\nControl for effects due to factors other than ones of primary interest. This can be done be blocking and stratification, which are both ways of allocating treatment randomly within relatively homogeneous sub-populations. For example, if there are differences in soil fertility across a large field, you want to create smaller blocks where the fertility is relatively constant, and allocate treatments within those blocks.\n\nReplication\n\nRepeat the experiment under identical conditions on multiple subjects. The more subjects you include, the lower the variability of your estimates. You can think of this as, the more subjects that exhibit similar behavior, the more confident you are of the results.\n\n\n\n\n\n\nThe Food and Agriculture Organization (FAO) of the United Nations provides a nice description of these principles\nThese principles are still used as a first step in designing any study, be it in the medical field, finance, customer analytics, and engineering.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Experimental design and confounding</span>"
    ]
  },
  {
    "objectID": "chapters/week-02.html#what-is-a-statistical-effect",
    "href": "chapters/week-02.html#what-is-a-statistical-effect",
    "title": "2  Experimental design and confounding",
    "section": "What is a statistical effect? \n",
    "text": "What is a statistical effect? \n\n\n\nThe  will denote sections that provide ideas and reflections on particular topics.\nWhen we think about statistical effect, we usually asssociate it with a statistical test that is used to evaluate whether the effect exists, or is “statistically significant”. Let’s put “significance” aside for the moment.\nLet’s look at some examples of test statistics. The 2-sample t-test statistic can be written as\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n\\]\nSimilarly, a test to see if a proportion is different from a particular \\(p_0\\) is evaluated using\n\\[\nZ = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n\\]\nIn these examples, and more generally, we see that the test statistic is measuring a signal-to-noise ratio, where the numerator is the signal (difference between groups) and the denominator is the standard error of the estimated signal.\nStatistical inference can be considered as a way of seeing if there is sufficient signal in the data to overcome the noise in the data, so the signal rises above the noise, so to speak. So, to find a statistical difference we need to have (a) the signal to be larger, or (b) the noise to be smaller. So, to me, statistics is really about variability; if we can find ways to reduce variability then we have a chance of finding something more than just massive effects. . This is valid not just for testing, but for confidence intervals as well; if there is lower noise, our confidence intervals will be shorter and we can make more precise statements about our estimatesKeep this in mind, as we go forward!\nThis really informs how we can think about experimental design. Thinking of the estimation problem (confidence intervals and the like), we want our estimate (signal) to be accurate (unbiased), and our variability (noise) to be low. Let’s relate this to the principles. What randomization gives us is minimizing bias,so our estimates are more likely to be unbiased. What local control and replication give us are means to look at estimates within subgroups with lower variance, which in turn reduces the standard error of the estimate. So we can be clever with local control and replication, so appropriate stratification and sufficient number of samples, we can reduce the variability of our estimate and so get more accurate estimates; the randomization piece tries to ensure that the estimates are on target.\n\n\n\n\n\n\nIn machine learning, you learned about the bias-variance trade-off, in that you can have simple learners that have high bias and low variance, or more complex learners that have low bias and high variance. You try to improve upon this by boosting (reduce bias) and bagging (reduce variance).\nFor problems of statistical association and causality, we still can use models that have good bias-variance properties, but the study design can help further to get better estimates, and we can account for the particulars of the study design in the models.\n\n\n\nThe case for replication in statistical hypothesis testing\nClassical statistical hypothesis testing, often referred to as Null Hypothesis Significance Testing (NHST), is typically formulated, in it’s simplest form, as\n\\[\nH_0: \\theta = \\theta_0 \\text{ vs } H_1: \\theta = \\theta_1\n\\]\nWe then compute a statistic and a criterion to evaluate if that difference exists. For example, if we are comparing 2 normal distributions with mean \\(\\mu_1\\) and \\(\\mu_2\\) and common known variance \\(\\sigma^2 = 0.25\\), then we can test whether the means are different or not. A simple set of hypotheses can be\n\\[\nH_0: \\mu_1 - \\mu_2 = 0 \\text{ vs } H_1: \\mu_1 - \\mu_2 = 0.5\n\\]\nIf we took \\(n\\) samples from the first distribution and \\(n\\) from the second distribution, and computed the sample means from each, the standard t-test would give us a decision rule that we would reject \\(H_0\\) if\n\\[\n\\sqrt{n}(\\bar{x}_1 - \\bar{x}_2)&gt; z_{1-\\alpha} \\sqrt{2}\\sigma\n\\tag{2.1}\\] This should look familiar as a 1-sided t-test, where \\(z_{1-\\alpha}\\) is the \\(100\\times (1-\\alpha)\\) percentile of the standard normal distribution (though slightly manipulated to make a point).\nThere are two kinds of mistakes that we consider in NHST: the type I error (reject \\(H_0\\) when \\(H_0\\) is true) and the type II error (do not reject \\(H_0\\) when \\(H_0\\) is false). We want to minimize the risk of these errors in our decision-making. This is the basis of the Neyman-Pearson lemma.\nLet’s look back at Equation 2.1. If \\(\\sigma\\) is bigger, you need to show a bigger difference in the means to reject \\(H_0\\). And, for a fixed \\(\\sigma\\), you have a better chance of rejecting \\(H_0\\) when \\(n\\) is large. Relating this to the error risks, we will find that the probability of type I error is \\(\\alpha\\) by the way the decision rule is constructed. The idea behind the Neyman-Pearson lemma is that, for a particular type I error, we will minimize the risk for the type II error, or, in other words, maximize the statistical power P(reject \\(H_0\\) when \\(H_0\\) is false).I’m not concerned so much with \\(\\bar{x}_1 - \\bar{x}_2\\) since the mean is unbiased, and so this difference will roughly be \\(\\mu_1 - \\mu_2\\)\nWhat is power measuring, qualitatively? It is measuring the degree of separation, or lack of overlap, between the sampling distributions of \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) when there is in truth a difference in means. If there is a lot of overlap, then the signal can’t be seen through the noise and the likelhood that you’ll reject \\(H_0\\) goes down. If the sampling distributions are separated, then the signal is clear despite the noise and you’re more likely to reject \\(H_0\\). You can increase your chances that the distributions separate if you can make the standard error of the means (\\(\\sigma/\\sqrt{n}\\)), smaller, which can happen if you make \\(n\\) larger.\n\n\n\n\n\n\nPlayground\n\n\n\nIn the following snippet, you can play around with m2, s and n to see how statistical power changes. What do you observe?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlayground\n\n\n\nNeed to develop a similar example for the benefits of local control/blocking",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Experimental design and confounding</span>"
    ]
  },
  {
    "objectID": "chapters/week-02.html#types-of-study-designs",
    "href": "chapters/week-02.html#types-of-study-designs",
    "title": "2  Experimental design and confounding",
    "section": "Types of study designs",
    "text": "Types of study designs\nThere are several study designs that are commonly used. The choice of study design to use for a particular study can depend on the time needed/available, resources available, degree of definitiveness desired, and, often, convenience.\nObservational studies\nObservational studies are just that – data is collected as it is observed. The interest is not in seeing particular differences between groups, but in seeing a snapshot of reality that may be indicative of existing differences. The purpose is to observe, not treat, and so what we’re looking for are associations or correlations between variables that we observe.\nObservational studies are often the first look into a problem, to see if an intervention might be needed. They generally fall under the umbrella of association studies, and are widely used in epidemilogic studies. Often such studies are designed to find links between some environmental or genetic factor and the risk of some disease, and are crucial to the study of why things might be happening.\n\nAssociation of smoking with lung cancer incidence among men\nAssociation of EGFR mutations with lung cancer and BRCA mutations with breast cancer\n\nCase-control studies\nMany of these associations were found by case-control studies, which are retrospective studies. In these studies, we first identify groups of individuals with and without the disease (often in equal numbers), and then observed what fraction of them had the factor of interest (say, EGFR mutations). These are retrospective studies since we are looking back from a time when a disease was caught to see if some exposure or experience was more likely (associated) among people with the disease compared to people without the disease.\nCase-control studies are a great design if you’re invesigating an outcome that is rare. For rare outcomes, if you started with a group of people and waited till they got the outcome (got diagnosed with lung cancer), you’d probably wait a long time, and wouldn’t have very many people with the outcome (cases) at the end of it. Instead, you start by finding people who already have the disease, match them with comparable people (think age, sex, ethnicity) who don’t have the disease (controls), and then see how often the exposure of interest happened in each group. You get more subjects with this method when you have a rare outcome, helping us with the replication principle. You also want to have some representativeness of the underlying population, so, once cases and controls are identified, study participants are often selected by random sampling from the population subgroups of cases and controls. If we’re including matched controls (a form of local control by having similar age, sex, race, for example), you randomly select, for each case, 1 or more matched controls.\nThe main hurdle in creating good case-control studies is in getting a good group of controls that are comparable to the cases and don’t bias your results. Wacholder, et al wrote a series of papers on this subject which form the basis of how controls are selected today.Disclosure: Wacholder was my mentor at the NCI during my postdoctoral training.\n\n\n\n\n\n\nFood for thought\n\n\n\n\nYou cannot measure the risk of getting a disease if you have a particular exposure from a case-control study. You need additional information from the study population. What do you need?\nYou can run machine learning models, in particular classification models, on case-control studies, that will classify subjects into cases and controls given their measured exposures. You can, for most such models, get output on the probability of being of a particular class. Is that probability generalizable to the population, i.e. can you use a ML model to help predict your chance of getting lung cancer?\nWe’ll learn more about the ways to establish causality next week. Know, for now, that you cannot ascribe causal relationships directly from a case-control study.\n\n\n\nCase-control studies are often summarised using the odds ratio. A case-control study can be described simply with a 2x2 table, where D +/- denotes if a subject has the disease, and E +/- denotes if they have the exposure of interest.\n\n\n\nD+\nD-\nTotal\n\n\n\nE+\na\nb\n\n\n\nE-\nc\nd\n\n\n\nTotal\n\n\n\n\n\n\nFrom this table, the odds ratio is computed as \\(ad/bc\\), or \\((a/c)/(b/d)\\), where \\(a/c\\) is the odds of exposure among the cases and \\(b/d\\) is the odds of exposure among the controls. For large samples, we can compute the variability of this statistic with the following formula:\n\\[\nvar \\log(OR) = \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\n\\]\nProvided you have sampled cases and controls in a representative (randomized) manner from the population, you can get estimates of P(E+ | D+) from a case-control study, because the sampling happens conditional on the disease state, but not P(D+ | E+), which is often of interest.\nCase-control studies are a more structured form of an observational study compared to just making observations, and so inferences are often stronger.\nCohort studies\nCohort studies are another form of structured observational study. Cohorts, as the name suggests, are groups of individuals collected at a point in time and followed forward in time. For representativeness, cohorts are selected by random sampling from a particular study population. Baseline measurements are taken and then the individuals are followed to see what develops, typically in terms of disease. These studies enable us to understand the natural history and evolution of disease in people, since we can observe their exposures/genetics and then see if they eventually get a disease.\nCohort studies are, by design, much longer in duration, since that time is required for the outcomes of interest to be observed. Many large cohort studies can form definitive associations between exposures and diseae (both due to the large sample size reducing the variability of estimates, and also because we can see cause-and-effect to some degree as we go forward in time), that lead to much better understanding of disease etiology. They can also serve to provide evidence against particular hypotheses. We will see how some cohort studies meet the Bradford-Hill criteria for causality, and so some causality can be inferred from them.\nAt a basic level, we can also summarise a cohort study with a 2x2 table.\n\n\n\nD+\nD-\nTotal\n\n\n\nE+\na\nb\n\n\n\nE-\nc\nd\n\n\n\nTotal\n\n\n\n\n\n\nYes, this is identical to the earlier table, but the understanding is, here the exposure came first. We can compute and genralize P(D+ | E+), the risk of getting a disease if you are exposed, in a well-designed cohort study. Cohort studies thus often report the risk ratio for an exposure, i.e., P(D+|E+) / P(D+|E-), i.e., how much more likely is one to get the disease if one experienced the exposure of interest, compared to if one didn’t.\nOne can also compute the odds ratio using the same formula as for case-control studies, and it will have the same interpretation. Why?\n\\[\nOR = ad/bc = (a/c)/(b/d) = (a/b)/(c/d)\n\\]\n\n\n\n\n\n\nThe magic utility of logistic regression\n\n\n\nLogistic regression is a common regression method where you model the prospective incidence of a disease on different exposures over a cohort. This is what logistic regression is primarily designed for.\nThe duality of the odds ratio as a summary measure for retrospective case-control studies and prospective cohort studies seems to indicate that we should be able to use logistic regression, which computes conditional odds ratios, in either study design. It’s a little trickier with logistic regression, since you are adjusting for other covariates.\nIt turns out, as shown by Prentice & Pyke (1979), that you can use logistic regression in case-control studies in the same way that you would in a prospective study, and you would get the same odds ratios and asymptotic variance estimates from either design. All that changes is the intercept term in the model, which we ignore anyway. \n\n\nR. L. PRENTICE, R. PYKE, Logistic disease incidence models and case-control studies, Biometrika, Volume 66, Issue 3, December 1979, Pages 403–411, https://doi.org/10.1093/biomet/66.3.403\n\n\n\n\n\nThe poorly understood and abused odds ratio\n\n\n\nAs we see, we use the odds ratio as a summary measure of association in both cohort and case-control studies. But what really is an odds ratio?\nThe odds of something is really quite an odd measure, though it is commonly used in gambling. The odds of an getting a disease D if exposed with exposure E is P(D+ | E+) / P(D- | E+). So the more likely an event, the higher the odds of it occurring. But we’re typically used to the probability P(D+|E+), the risk of an event when exposed, not it’s odds. We might like to understand the risk ratio or relative risk of the disease, which is P(D+ | E+) / P(D+ | E-), or the proportional increase in risk when exposed compared to when not exposed. The corresponding odds ratio is P(D+ | E+) P(D- | E-) / p(D+|E-)P(D-|E+). So the odds ratio and risk ratio are not the same! From the 2x2 table, the OR is ad/bc, and the RR is (a/(a+b))/(c/(c+d)).\nIf you peruse the medical literature, you’ll see lots of odds ratios, mainly because we know how to do logistic regression and the equivalence of the logistic regression results in prospective and retrospective studies. There’s no corresponding regression method for risk ratios, simply because valid risk ratios can’t be computed in retrospective studies. You’ll see, however, that odds ratios are regularly interpreted as risk ratios!. Is this valid? Yes, in special situations. If you have a rare disease (so chances of getting the disease is low, like most cancers), then the odds ratio and risk ratio are roughly the same, and so the interpretation holds. If the rare disease assumption does not hold (like, for example, diabetes), then the odds ratio will overestimate the relative risk.\nYou might also wonder why, in logistic regression, we get the log of the odds ratio, other than as a mathematical by-product. That mathematics is to make things symmetric, which has computational and interpretation advantages. The log-odds is symmetric around 0, and so positive and negative differences in log-odds are computable and interpretable as equivalent to proportional changes in the odds ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandomized controlled studies\nRandomized controlled studies are the gold standard for studies to ascertain causal relationships. They are usually used to evaluate interventions and how they affect an outcome. They use all the principles of good study design to allow definitive conclusions to be made. RCTs randomize individuals to groups, with each such group getting a different intervention. The randomization allows us to balance measured and unmeasured confounders and minimize systematic biases, and allows us to attribute differences in outcomes between groups to differences in treatments.\nWe will come back to RCTs in more detail in both next week and in Chapter 10.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Experimental design and confounding</span>"
    ]
  },
  {
    "objectID": "chapters/week-02.html#correlation-is-not-causation-and-confounding",
    "href": "chapters/week-02.html#correlation-is-not-causation-and-confounding",
    "title": "2  Experimental design and confounding",
    "section": "“Correlation is not causation”, and confounding",
    "text": "“Correlation is not causation”, and confounding\nMost studies we’ve described will be able to extract correlations or associations between an exposure and disease incidence. But, other than RCTs, we need to be careful that the observed correlations aren’t due to other factors.\nThere are many examples of spurious correlations that can be found if one tortures data long enough. We will ignore those and focus on situations where the correlations are reasonable. One reason that we might see correlations between variables but no direct causal relationship is due to the presence of a confounder.\n\nConfounder\n\nA variable that is associated with both the exposure and the disease, and creates the appearance of a correlation between them.\n\n\n\nConfounders are one of the things we have to worry about when we’re assessing associations. And its not just the confounders we measure, it’s also confounders that we didn’t measure. This requires thought, and some critical thinking. This is not necessary an analytic thing, but often requires domain knowledge.\nWe’ll come back to this next week in more detail. It is one of the foundational issues in assessing causality.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Experimental design and confounding</span>"
    ]
  },
  {
    "objectID": "01-technical-background.html",
    "href": "01-technical-background.html",
    "title": "Appendix A — Technical background",
    "section": "",
    "text": "Software\nIn this class, we will use  as our primary software. We expect you to be familiar with the tidyverse packages, and ggplot2 for visualization.\nIn this class, we will make extensive use of the survival package, as well as various specialized packages",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical background</span>"
    ]
  },
  {
    "objectID": "01-technical-background.html#probability-distributions",
    "href": "01-technical-background.html#probability-distributions",
    "title": "Appendix A — Technical background",
    "section": "Probability distributions",
    "text": "Probability distributions\nYou are probably familiar with the normal (Gaussian) distribution and the binomial distribution. We’ll introduce some new distributions that will be used in the class\n\nExponential distribution\nThe exponential distribution has one parameter, usually denoted by \\(\\lambda &gt; 0\\) , which is also the mean of the distribution. If \\(X\\sim Exp(\\lambda)\\), then\n\\[\nP(X &gt; x) = e^{-\\lambda x}\\quad \\text{for } x &gt; 0\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWeibull distribution\nThe Weibull distribution is a generalization of the Exponential distribution. It is a two-parameter distribution, with the parameters called shape (\\(\\alpha\\)) and scale ( \\(\\beta\\) ). For this distribution,\n\\[\nP(X &gt; x ) = e^{-(x/\\beta)^\\alpha}\n\\]\nNotice that the Exponential distribution is a special case with \\(\\alpha = 1\\) and \\(\\beta\\) = \\(1/\\lambda\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nGamma distribution\n\n\nPoisson distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical background</span>"
    ]
  },
  {
    "objectID": "01-technical-background.html#statistical-methods",
    "href": "01-technical-background.html#statistical-methods",
    "title": "Appendix A — Technical background",
    "section": "Statistical methods",
    "text": "Statistical methods\n\nHypthesis testing and errors",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Technical background</span>"
    ]
  }
]